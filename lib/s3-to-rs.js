/**
 * Created by teklof on 30.1.15.
 */
const $ = require('highland');
const _ = require('lodash');
const util = require('util');
const config = require('config');
const ut = require('./utils');
const Promise = require('bluebird');
const aws = require('aws-sdk');
const pause = require('promise-pauser');
const ZabbixSender = require("zabbix-sender");
const LRU = require("lru-cache");
const co = require("co");

const p = require('./sqs-poller');
const mup = require('./manifest-uploader');
const Statsd = require('statsd-client');
const instr = require('./instrumentation');
const hc = require('./health-checker');
const Uploader = mup.Uploader;
const inspect = _.partialRight(util.inspect, {depth: 2});

/**
 * S3Copier polls SQS for S3 object creation events and uses COPY to copy the S3 object data to Redshift.
 *
 * Don't forget to provide credentials in copyParams.
 *
 * @param {Object} s3 Initialized AWS S3 service object
 * @param {Object} pg Postgres module
 * @param {Object} rs Initialized AWS Redshift service object
 * @param {Object} poller SQS poller
 * @param {Number} options.pollIntervalSeconds Interval between SQS polls
 * @param {Object} options.healthCheck Pipeline health check configuration
 * @param {String} options.connStr Connection string for Redshift
 * @param {String} options.tablePostfix Added to the end of the table name generated by
 * copyParams.table. Handy for having different tables based on NODE_ENV
 * @param {Number} options.clusterAvailCheckInterval Check cluster status at clusterAvailCheckInterval ms intervals.
 * If the cluster is not available, SQS polling and manifest uploading will be stopped.
 * @param {Object} options.manifestUploader Manifest uploader options. See ./manifest-uploader.js for details.
 *
 * @param {Object} options.tableConfig Time series table configuration
 * @param {Number} options.tableConfig.period Time series table period in seconds. I.e. a value of 3600 would mean
 * a new time series table is created per every hour
 * @param {Number} options.tableConfig.maxTables Keep a maximum of maxTables time series tables. Oldest tables will be
 * deleted
 * @param {Object} options.tableConfig.tableDef Time series table DDL etc.
 * @param {Array} options.tableConfig.tableDef.columns Array of column definitions like ["source int not null encode bytedict",
 * "campaign char(24) encode bytedict", "..."]
 * @param {Array} options.tableConfig.tableDef.tableAttrs Array of table attributes like ["DISTKEY(ID)", "SORTKEY(TS)"]
 * @constructor
 * @type {S3Copier}
 */
const S3Copier = exports.S3Copier = function S3Copier(pg, s3, rs, options) {
  if(!options) {
    throw new Error("Missing options");
  }

  if(!pg) {
    throw new Error("Missing PostgreSQL module");
  }

  if(!options.logger) {
    throw new Error("no logger");
  }

  if(!options.Redshift.connStr) {
    throw new Error("missing connection string");
  }

  if(!options.Redshift.params.ClusterIdentifier) {
    throw new Error("missing options.Redshift.params.ClusterIdentifier");
  }

  if(!options.manifestUploader) {
    throw new Error("missing manifest uploader options");
  }

  if(!rs) {
    throw new Error("Missing Redshift service");
  }

  if(!s3) {
    throw new Error("Missing S3 service")
  }

  if(!options.tableConfig) {
    throw new Error("Missing table configuration");
  }

  const copyParams = options.copyParams;

  const table = copyParams.table;
  if(!_.isString(table) && !_.isFunction(table)) {
    throw new Error("copyParams.table must be a string or function");
  }

  let _tableNamer;

  if(_.isString(table)) {
    _tableNamer = ut.tableStrToNamer(table);
  } else {
    _tableNamer = table;
  }

  this._tableNamer = _tableNamer;

  this._options = options;

  this._logger = options.logger.child({module: "S3Copier"});

  this._pollIntervalS = options.pollIntervalSeconds || 60;

  this._availCheckInterval = options.Redshift.clusterAvailCheckInterval || -1;

  if(options.tablePostfix) {
    this._logger.info({postfix: options.tablePostfix}, "Using a table postfix");
  }

  this._tablePostfix = options.tablePostfix || "";

  if(!copyParams.args) {
    copyParams.args = [];
  }

  copyParams.args.push("MANIFEST");

  this._commonCopyParams = copyParams;

  if(options.healthCheck) {
    this._logger.info({healthCheck: options.healthCheck}, "Pipeline health checking enabled");
    options.healthCheck.logger = this._logger;
    this._healthChecker = new hc.HealthChecker(options.healthCheck)
  }

  this._pauser = pause.pauser();
  this._pauser.pause();

  this._pg = pg;
  this._connStr = options.Redshift.connStr;
  this._baseToTempl = {};

  this._rs = Promise.promisifyAll(rs);
  this._started = false;
  this._availChk = null; // used for cluster availability check interval

  // if an _onManifest call is running, this'll contain an object with properties that'll be fulfilled when the messages
  // have been handled. Keys will be table names
  this._manifestsPending = {};

  this._dbTimeout = options.Redshift.timeout || (1000 * 60 * 15);

  const topStatsd = new Statsd(options.statsd);

  let zabbixSender;

  if(options.zabbix) {
    zabbixSender = new ZabbixSender(options.zabbix);
  }

  const baseTables = _.filter(_.keys(options.tableConfig), (key) => !!options.tableConfig[key].columns);

  this._URISeenCache = LRU(options.LRU || {max: 3000});

  // since _tableNamer takes an S3 URI and turns it into a table name, we can use it as the manifest uploader's grouper
  this._uploader = new Uploader(s3, _.merge(options.manifestUploader, {logger: this._logger, grouper: _tableNamer, statsd: options.statsd, baseTables: baseTables}));

  this._instrumenter = new instr.Instrumenter(topStatsd, "s3-to-rs", zabbixSender);

  this._errInstr = new instr.Instrumenter(this._instrumenter, "errors");
  this._errIncr = this._errInstr.instrumenterFn("increment", "top");

  this._usingPg = (logger, fn) => Promise.using(ut.getPgClient(this._pg, this._connStr, logger), fn);

  this._tblMgr = new TableManager(this._usingPg, copyParams.schema, this._tablePostfix, this._logger, options.tableConfig);

  this.errorStream = $();

  this._setupPollers(options);

};

S3Copier.prototype._setupPollers = function _setupPollers(options) {
  const errHandler = (err) => {
    // CancellationErrors need to be ignored so S3Copier.stop() can finish cleanup
    if (err instanceof Promise.CancellationError) {
      return;
    }

    this._errIncr();
    this._logger.error({err: err}, `_unseenStream encountered an unhandled error`);
    this.errorStream.write(err); // yeah yeah, ugly hack
  };

  const notSeen = msg => {
    const isSeen = _.any(msg.toURIs(), this._URISeenCache.get.bind(this._URISeenCache));
    return !isSeen;
  };

  const commonSqsConf = config.has('SQS') ? config.get('SQS') : {}
    , baseTables = _.filter(_.keys(options.tableConfig), (key) => !!options.tableConfig[key].columns);

  this._logger.info({baseTables: baseTables}, "Setting up SQS pollers");

  this._pollers = _.map(baseTables, baseName => {
    const baseSqsConf = options.tableConfig[baseName].SQS || {}
      , sqsConf = _.merge(baseSqsConf, commonSqsConf)
      , pollerOpts = sqsConf.poller || {};

    pollerOpts.filter = ut.nameFilterFnFor([baseName], this._tableNamer);
    pollerOpts.statsd = options.statsd;
    pollerOpts.logger = this._logger;

    this._logger.info({baseName: baseName}, "Creating SQS poller");

    const sqs = new aws.SQS({region: sqsConf.region, params: sqsConf.params})
      , poller = new p.Poller(sqs, pollerOpts);

    // stream messages from the poller to the manifest uploader's input stream for this base name

    poller.messageStream.fork()
      .flatMap($.compose($, pause.waitFor(this._pauser)))
      // only pass through messages that haven't been COPY'd already
      .filter(notSeen)
      // partially apply msgsToManifests with the baseName, resulting in a function that takes a stream, and then pass
      // that to through()
      .through(_.partial(this._uploader.msgsToManifests.bind(this._uploader), baseName))
      // msgsToManifests returns a Stream[Stream[Manifest]] (i.e. a stream of streams of manifests). "Unwrap" it
      // with sequence() so we end up with just a stream of Manifests
      .sequence()
      // feed the Manifests to _onManifest which returns a Promise of a Manifest. "Unwrap" that Promise with flatMap
      .flatMap(this._onManifest.bind(this))
      .errors(errHandler)
      .each((mf) => this._logger.info({baseName: baseName, manifestURI: mf.manifestURI}, "Finished handling manifest"))
    ;


    poller.messageStream.observe()
      // only pass through messages that _have_ been successfully COPY'd, i.e. duplicates
      .filter(msg => _.any(msg.toURIs(), this._URISeenCache.get.bind(this._URISeenCache)))
      .tap((msg) => this._logger.warn({uris: msg.toURIs()}, "Deleting message that was seen before"))
      .each(ut.send("deleteMsg"));

    return poller;
  });
};

S3Copier.prototype._shouldDedup = function _shouldDedup(base) {
  return !!this._dedupConfsFor(base);
};

S3Copier.prototype._dedupConfsFor = function _dedupConfsFor(base) {
  return this._options.tableConfig[base].deduplication;
};

S3Copier.prototype._cleanDedup = function _cleanDedup(client, table, base, latestFinalized) {
  const dedupConf = this._dedupConfsFor(base)
    , setAge = dedupConf.dedupSetAge || "1 day"
    ;

  client.txLogger.debug({baseName: base, dedupSetAge: setAge, latestFinalized: latestFinalized}, "Cleaning dedup table");

  const query = `DELETE FROM ${table}_dedup WHERE processing_time < '${latestFinalized}'::timestamp - interval '${setAge}'`;

  return client.queryAsync(query)
    .tap((res) => client.txLogger.debug({rowCount: res.rowCount, baseName: base}, `Dedup table cleaned`))
    .timeout(this._dbTimeout, `_cleanDedup table ${table} base ${base}`)
    ;
};

S3Copier.prototype._updateTracking = function _updateTracking(client, table, base, stagingTbl, latestFinalized) {
  if(this._healthChecker && !this._healthChecker.healthOk) {
    client.txLogger.warn({baseName: base}, `Pipeline health is bad, not updating finalization time`);
    return;
  }

  const dedupConf = this._dedupConfsFor(base);
  client.txLogger.debug({baseName: base, dropOlderThan: dedupConf.dropOlderThan, latestFinalized: latestFinalized}, "Updating tracking table");

  const query = `INSERT INTO ${table}_tracking (processing_time, finalized_time) (
      SELECT sysdate, GREATEST(MAX(${this._options.tableConfig[base].timestampCol}) - interval '${dedupConf.dropOlderThan}', '${latestFinalized}'::timestamp) FROM ${stagingTbl})`;

  return client.queryAsync(query);
};

S3Copier.prototype._updateDedupTbl = function _updateDedupTbl(client, table, base, stagingTbl) {
  const pk = this._tblMgr.pkFor(base).name
    , query = `INSERT INTO ${table}_dedup `
              + `(SELECT DISTINCT ${stagingTbl}.${pk}, sysdate as processing_time FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup USING (${pk}) `
              + `WHERE ${table}_dedup.${pk} IS NULL)`
    ;

  client.txLogger.debug({baseName: base, stagingTbl: stagingTbl}, "Updating dedup table");

  return client.queryAsync(query).tap((res) => client.txLogger.debug({rowCount: res.rowCount, baseName: base}, "Dedup table update done"))
    .get("rowCount")
    .timeout(this._dbTimeout, `_updateDedupTbl table ${table} base ${base} staging ${stagingTbl}`)
    ;
};

S3Copier.prototype._badToErrTbl = function _badToStaging(client, table, base, stagingTbl, latestFinalized) {
  client.txLogger.debug({baseName: base, stagingTbl: stagingTbl, latestFinalized: latestFinalized}, "Inserting duplicates and old events into error table");
  const pk = this._tblMgr.pkFor(base).name
    , tableConfs = this._options.tableConfig[base]
    , columns = _(tableConfs.columns) // get column definitions
    // split each definition into words delimited by spaces, then get the first element of the resulting arrays
    .map(_.flow(_.partial(_.words, _, /\S+/g), _.property("0"), _quote)).valueOf()
    , tsc = this._options.tableConfig[base].timestampCol
    , query = `INSERT INTO ${table}_errors (dedup_id, processing_time, ${columns.join(", ")}) `
              + `(SELECT ${table}_dedup.${pk} AS dedup_id, sysdate as processing_time, ${stagingTbl}.* FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup USING (${pk}) `
              + `WHERE ${table}_dedup.${pk} IS NOT NULL OR ${stagingTbl}.${tsc} < ('${latestFinalized}'::timestamp))`
    ;

  return client.queryAsync(query).tap((res) => client.txLogger.debug({rowCount: res.rowCount, baseName: base}, "Handled duplicate/old rows"))
    .get("rowCount")
    .timeout(this._dbTimeout, `_badToErrTbl table ${table} base ${base} staging ${stagingTbl}`)
    ;
};

function _quote(s) {
  return `"${s}"`
}

S3Copier.prototype._stagingToActual = function _stagingToActual(client, table, base, stagingTbl, latestFinalized) {
  client.txLogger.debug({baseName: base, stagingTbl: stagingTbl, latestFinalized: latestFinalized}, "Inserting from staging to actual table");

  const tableConfs = this._options.tableConfig[base]
    , columns = _(tableConfs.columns) // get column definitions
    // split each definition into words delimited by spaces, then get the first element of the resulting arrays
    .map(_.flow(_.partial(_.words, _, /\S+/g), _.property("0"), _quote)).valueOf()
    , pk = this._tblMgr.pkFor(base).name
    , query = `INSERT INTO ${table} SELECT ${columns.join(", ")} FROM `
              +
              `(SELECT ${stagingTbl}.*, ROW_NUMBER() OVER (PARTITION BY ${stagingTbl}.${pk}) AS id_rank, dedup.${pk} AS dedup_id `
              + `FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup AS dedup USING (${pk})) as staging `
              +
              `WHERE id_rank = 1 and dedup_id IS NULL AND staging.${tableConfs.timestampCol} >= ('${latestFinalized}'::timestamp)`
    ;

  return client.queryAsync(query).tap((res) => client.txLogger.debug({baseName: base, rowCount: res.rowCount}, "Inserted from staging to actual table"))
    .get("rowCount")
    .timeout(this._dbTimeout, `_stagingToActual table ${table} base ${base} staging ${stagingTbl}`);
};

S3Copier.prototype._copyTemplateFor = function(base) {
  if(!this._baseToTempl[base]) {
    const paramsForBase = _.get(this._options.tableConfig[base], "copyParams") || {};
    const finalParams = _.defaultsDeep(_.cloneDeep(paramsForBase), _.cloneDeep(this._commonCopyParams));
    this._baseToTempl[base] = _copyParamsTempl(finalParams);
  }
  return this._baseToTempl[base];
};

S3Copier.prototype._connAndCopy = function _connAndCopy(s3URI, base, table) {

  let cancelCopy
    , txCtx = "COPY for " + s3URI
    , logger = this._logger.child({baseName: base, table: table})
    , ins = new instr.Instrumenter(this._instrumenter, "connAndCopy." + base)
    ;

  logger.trace("Starting to copy data from S3");

  const _copyTo = (client, dest) => {
    const query = util.format(this._copyTemplateFor(base), dest, s3URI);
    client.txLogger.info({manifest: s3URI, dest: dest}, "Starting COPY query");
    return client.queryAsync(query).return(s3URI).timeout(this._dbTimeout, `COPY from ${s3URI} to ${dest}`)
      .tap(() => client.txLogger.info({manifest: s3URI}, "COPY done"));
  };

  const copyTo = ins.instrumentCalls("copyTo", _copyTo);

  const _dedupCopy = co.wrap(function*(client) {
    client.txLogger.info("Doing deduplication");

    const dedupConf = this._dedupConfsFor(base);

    const okCounter = ins.usingInstrFn("counter", "okrows", _.identity)
      , badCounter = ins.usingInstrFn("counter", "badrows", _.identity);

    // split table name (which includes the schema) by . and take last part since temporary tables have their own schema
    const stagingTbl = `${_.last(table.split("."))}_staging_${ut.randomString(5)}`;

    const stagingDdl = `CREATE TEMPORARY TABLE ${stagingTbl} (LIKE ${table})`;

    let finalizedResult = yield client.queryAsync(
      (`SELECT COALESCE(MAX(finalized_time), sysdate - interval '${dedupConf.dropOlderThan}') as t FROM ${table}_tracking`))
      .timeout(this._dbTimeout, `Get last finalized timestamp for ${base}`);


    const latestFinalized = ut.redshiftDateFix(_.get(finalizedResult, "rows[0].t")).toISOString();
    client.txLogger.trace({latestFinalized: latestFinalized}, "Found latest finalization time")

    client.txLogger.debug({stagingTbl: stagingTbl}, "Creating staging table");
    yield client.queryAsync(stagingDdl).timeout(this._dbTimeout, `Create staging table for ${base}`);
    yield copyTo(client, stagingTbl);

    // we can simultaneously move good rows from the staging table to the actual one, and bad rows to the errors table
    yield [this._stagingToActual(client, table, base, stagingTbl, latestFinalized).tap(okCounter)
      , this._badToErrTbl(client, table, base, stagingTbl, latestFinalized).tap(badCounter)];

    // these have to be done after the ones above since they alter the dedup table's contents, but they can still
    // run in parallel
    yield [this._updateDedupTbl(client, table, base, stagingTbl, latestFinalized)
      , this._updateTracking(client, table, base, stagingTbl, latestFinalized)
      , this._cleanDedup(client, table, base, latestFinalized)];

    client.txLogger.info("Deduplication done");

    return s3URI;
  }.bind(this));

  const dedupCopy = ins.instrumentCalls("dedupCopy", _dedupCopy);

  return this._usingPg(logger, ins.instrumentCalls("total", (client) =>
      ut.inTransaction(client, txCtx, (txClient) => {
        cancelCopy = () => ut.cancelClient(txClient);
        if (this._shouldDedup(base)) return dedupCopy(txClient);
        else return copyTo(txClient, table).return(s3URI);
      })
    , {count: true, zabbix: true}))
    .cancellable()
    .tap(() => {
      cancelCopy = null;
    })
    .catch(Promise.CancellationError, (e) => {
      if(cancelCopy) {
        cancelCopy();
      }

      logger.error({manifest: s3URI},"_connAndCopy cancelled");
      throw e;
    });
};

S3Copier.prototype._deleteMsgsFor = function(manif) {
  const msgs = manif.msgs;
  this._logger.debug({baseName: manif.baseName, nMsgs: msgs.length}, "Scheduling messages for deletion");
  if(this._logger.debug()) {
    const chunked = _.chunk(msgs, 20);
    _.each(chunked, chunk => this._logger.debug({uris: ut.messagesToURIs(chunk)}, "URIs scheduled for deletion"));
  }

  _.each(msgs, msg => msg.deleteMsg());
};

S3Copier.prototype._markAsSeen = function(manif) {
  _.each(manif.msgs, msg => _.each(msg.toURIs(), uri => this._URISeenCache.set(uri, true)));
};

S3Copier.prototype._onManifest = function _onManifest(manif) {
  const logger = this._logger.child({manifestURI: manif.manifestURI, baseName: manif.baseName});

  logger.info("Got new manifest");

  // replacing this with a co.wrap + generator function combo breaks cancelability, so this'll have to be as-is
  // pending a complete rewrite to a newer version of Bluebird with coroutine support & revamped cancellation
  const handle = () => {
    // partially apply _connAndCopy so the resulting function only needs a table name
    const doCopy = _.partial(this._connAndCopy.bind(this), manif.manifestURI, manif.baseName);

    return this._tblMgr.tableFor(manif.baseName)
      .cancellable()
      .then(doCopy)
      .tap(() => this._markAsSeen(manif)) // mark all messages in a successfully copied manifest as seen
      .tap(() => {
        logger.info("Files in manifest copied to Redshift");
      })
      .then(() => manif.done(true).catch((err) =>
        logger.warn({err: err}, "Error moving manifest to success folder")))
      .then(() => {
        logger.debug("Deleting messages for OK manifest");
        this._deleteMsgsFor(manif);
      })
      .catch((err) => {
        logger.error({err: err}, "Error handling manifest");

        _.each(manif.msgs, m => m.stopVisibilityUpd());

        return manif.done(false).catch((err) => logger.warn({err: err}, "Couldn't move manifest to failure folder")).tap(() => {
          // if the error caught by the upper-level catch was a CancellationError or TimeoutError, cause this promise
          // to be rejected with it instead of ignoring it
          if (err instanceof Promise.CancellationError || err instanceof Promise.TimeoutError) {
            throw err;
          }
        });
      })
      .return(manif)
    ;
  };

  let pendingPromise;

  // NOTE: this shouldn't happen when using separate SQS queues
  if(this._manifestsPending[manif.baseName] && this._manifestsPending[manif.baseName].isPending()) {
    logger.fatal("Already have a pending manifest upload for this table? This shouldn't happen; bailing out");
    pendingPromise = this._manifestsPending[manif.baseName].then(handle);
  } else {
    pendingPromise = handle();
  }

  this._manifestsPending[manif.baseName] = pendingPromise;

  return $(pendingPromise);

};

/**
 * Stop the S3Copier. Returns a promise that'll be resolved once all underway COPYs are done.
 * @return {*}
 */
S3Copier.prototype.stop = function() {
  if(this._started) {
    // set this so that if _availHandler is waiting for its promise it won't try to start S3Copier again when it sees
    // it isn't started.
    this._stopping = true;
    this._started = false;

    this._logger.info("Stopping");

    _.each(this._pollers, p => p.stop());

    if(!this._pauser.paused) {
      this._pauser.pause();
    } else {
      this._logger.fatal({pauser: pauser}, "Trying to stop when not actually running?!");
    }

    clearInterval(this._availChk);
  }

  if(this._numPending() > 0) {
    return Promise.settle(_.map(_.values(this._manifestsPending), (promise) => {
      if (promise.isPending()) {
        return promise.cancel();
      } // TODO: add timeout
      return null;
    }))
      .tap(() => {
        this._logger.info("Cancel done");
      });
  }

  return Promise.resolve({});
};

S3Copier.prototype._numPending = function _numPending() {
  return _.reduce(this._manifestsPending, (acc, val) => acc + (val.isPending() ? 1 : 0), 0);
};

S3Copier.prototype._availHandler = co.wrap(function* _availHandler() {
  this._logger.debug("Checking cluster availability");
  let avail = yield this._isClusterAvail();
  if (!avail) {
    if (this._started) {
      this._logger.fatal("_availHandler: cluster went unavailable, bailing out");
      throw new Error("Cluster unavailable");
    }
  } else { // cluster available
    if (!this._started && !this._stopping) {
      this._logger.info("_availHandler: cluster became available, starting");
      return this.start(true);
    }
  }
});

S3Copier.prototype._isClusterAvail = co.wrap(function* _isClusterAvail() {
  let describeResult;

  try {
    describeResult = yield this._rs.describeClustersAsync();
  } catch (err) {
    this._logger.error({err: err}, "Error checking cluster availability");
    return false;
  }

  if (describeResult.Clusters.length != 1) {
    throw new Error("DescribeClusters got " + describeResult.Clusters.length + " results: did you forget to set " +
                    "Redshift.params.ClusterIdentifier?");
  }

  this._logger.debug({ClusterStatus: describeResult.Clusters[0].ClusterStatus}, "Availability check done");

  return describeResult.Clusters[0].ClusterStatus === "available";
});

/**
 * Starts the S3Copier
 * @param {Boolean} skipAvailCheck if skipAvailCheck is true, skip cluster availability check and just start.
 */
S3Copier.prototype.start = co.wrap(function* start(skipAvailCheck) {
  if (this._started) {return Promise.resolve()}

  let avail = skipAvailCheck ? true : yield this._isClusterAvail();

  if (avail) {
    this._logger.info("Cluster available, ready to start");
    yield [this._tblMgr.createMetadataTbls(), this._tblMgr.createMonolithicTbls()];
    this._pauser.unpause();
    _.each(this._pollers, p => p.start());
    this._started = true;

    if (this._availCheckInterval > 0 && !this._availChk) {
      this._availChk = setInterval(this._availHandler.bind(this), this._availCheckInterval * 1000);
    }

    if (this._healthChecker) {
      this._logger.info("Starting pipeline health checker");
      yield this._healthChecker.start();
    }

    this._logger.info("Started")

  } else {
    this._logger.error("Cluster unavailable, can't start yet");
    if (this._availCheckInterval < 0) {
      throw new Error("Cluster unavailable and no clusterAvailCheckInterval configured, bailing out");
    }

    // The interval can't be started before createMonolithicTbls and createMetadataTbls are finished, since
    // they might take longer than _availCheckInterval to complete, meaning start() would get called again
    // because we're not started yet
    if (!this._availChk) {
      this._availChk = setInterval(this._availHandler.bind(this), this._availCheckInterval * 1000);
    }
  }
});

const _boolish = /^(true|on|false|off)$/i;
const _enc = /^encoding$/i;

const _copyParamsTempl = exports._copyParamsTempl = function _copyParamsTempl(copyParams) {

  copyParams.withParams = copyParams.withParams || {};
  copyParams.args = copyParams.args || [];

  const withp = _.reduce(_.pairs(copyParams.withParams), (acc, pair) => {
    const key = pair[0];
    let value = pair[1];
    if (!(_.isNumber(value) || value.toString().match(_boolish) || key.match(_enc))) {
      // we need quotes since value is not a number or boolean-ish, and the key isn't "encoding"
      value = "'" + value + "'";
    }
    acc.push(key + " " + value);

    return acc;

  }, []);

  return "COPY %s FROM '%s' " + copyParams.args.concat(withp).join(' ') + ";";
};

/**
 * Creates new time series tables if needed, creates views for the tables, and deletes old tables.
 * @param {Function} usingPgFn A function that takes another function which will be called with a promisified Postgres client
 * and returns a promise of a result
 * @param {String} postfix A table postfix to be added to the base name _before_ the time series postfix.
 * @constructor
 */
const TableManager = exports.TableManager = function TableManager(usingPgFn, schema, postfix, logger, options) {

  if(!_.isString(postfix)) {
    throw new Error("postfix not a string");
  }

  if(!schema) {
    throw new Error("missing schema");
  }

  if(!_.isFunction(usingPgFn)) {
    throw new Error("Postgres client getter not a function");
  }

  if(!logger) {
    throw new Error("No logger");
  }

  if(!options) {
    throw new Error("no options");
  }

  this._options = options;

  this._baseNames = _.filter(_.keys(options), (key) => !!options[key].columns);

  this._defaultSchema = schema;

  this._postfix = postfix;

  this._usingPg = usingPgFn;

  this._logger = logger.child({module: "TableManager"});

  this._dbTimeout = options.timeout || (10 * 60 * 1000);

};

function isDuplicate(error) {
  return error && error.code === "42P07";
}

const _pkRe = /\bprimary key\b/i;

/**
 * Returns the name and type of the primary key for a base table
 * @param {String} base base table name
 * @return {{name: String, type: String}}
 */
TableManager.prototype.pkFor = function(base) {
  const cols = this._options[base].columns;
  for(let i = 0; i < cols.length; ++i) {
    if(_pkRe.test(cols[i])) {
      const colParts = _.words(cols[i], /\S+/g);
      return {name: colParts[0], type: colParts[1]};
    }
  }

  throw new Error("Couldn't find a PRIMARY KEY column in tableConfig for " + base);
};

TableManager.prototype._createDedupTbl = function(client, base) {
  const pkInfo = this.pkFor(base)
    , baseSchema = _.get(this._options[base], "copyParams.schema")
    , schema = baseSchema ? baseSchema : this._defaultSchema
    , table = `${schema}.${base}${this._postfix}_dedup`;

  client.txLogger.info({dedupTable: table}, `Creating dedup table`);

  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (${pkInfo.name} ${pkInfo.type} DISTKEY, processing_time TIMESTAMP SORTKEY)`);
};

TableManager.prototype._createTrackingTbl = function(client, base) {
  const table = `${this._defaultSchema}.${base}${this._postfix}_tracking`;

  client.txLogger.info({trackingTable: table}, "Creating tracking table");

  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (processing_time TIMESTAMP ENCODE DELTA, finalized_time TIMESTAMP SORTKEY)`);
};

TableManager.prototype._createErrorTbl = function(client, base) {
  const pkInfo = this.pkFor(base)
    , table = `${this._defaultSchema}.${base}${this._postfix}_errors`;

  client.txLogger.info({errorTable: table}, `Creating error table`);
  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (dedup_${pkInfo.name} ${pkInfo.type}, processing_time TIMESTAMP, handled_ts TIMESTAMP, ${this._options[base].columns.join(", ")})`);

};

TableManager.prototype.createMonolithicTbls = Promise.method(function () {
  const monoTbls = _.filter(this._baseNames, (base) => {
    const opts = this._options[base];
    return !(opts.period && opts.maxTables && opts.tablesInView);
  });

  if(monoTbls.length === 0) {
    this._logger.info("No monolithic tables configured for any base table");
    return;
  }

  this._logger.info({monoTbls: monoTbls}, "Setting up monolithic tables");

  return this._usingPg(this._logger, (client) =>
      ut.inTransaction(client, "createMonolithicTbls", (txClient) =>
        Promise.map(monoTbls, (base) => {
          const opts = this._options[base];
          const nameWithPostf = `${this._defaultSchema}.${base}${this._postfix}`;
          client.txLogger.info({tableName: nameWithPostf}, "Creating table");
          return txClient.queryAsync(
            `CREATE TABLE IF NOT EXISTS ${nameWithPostf} (${opts.columns.join(',')}) ${opts.tableAttrs.join(' ')}`);
        })))
    .timeout(this._dbTimeout, "createMonolithicTbls");
});

TableManager.prototype.createMetadataTbls = Promise.method(function() {
  const tablesWithDedup = _.filter(this._baseNames, (base) => !!this._options[base].deduplication);

  if(tablesWithDedup.length == 0) {
    this._logger.info("No deduplication configured for any base table");
    return null;
  }

  this._logger.info({tablesWithDedup: tablesWithDedup}, "Setting up deduplication metadata tables");
  return this._usingPg(this._logger, (client) =>
    ut.inTransaction(client, "createMetadataTbls", (client) =>
      Promise.map(tablesWithDedup, (base) =>
        Promise.join(
          this._createTrackingTbl(client, base)
          , this._createErrorTbl(client, base)
          , this._createDedupTbl(client, base)
        ))))
    .timeout(this._dbTimeout, "createMetadataTbls");
});

/**
 * Finds time series tables for a base name and returns them in a promise of an array
 * @param name base name
 * @return {Promise} promise of array of table names, sorted oldest first
 * @private
 */
TableManager.prototype._listTsTables = function(name) {
  return this._usingPg(this._logger, (client) => {
    const queryStr = "select tablename from pg_tables where schemaname = $1" +
                   " and tablename like $2 || '_ts_%' order by tablename asc"
      , params = [this._defaultSchema, name + this._postfix]
      ;

    return client.queryAsync(queryStr, params)
      .then(function (res) {
        return _.pluck(res.rows, "tablename");
      });
  }).catch((e) => {
    this._logger.error("_listTsTables error: " + e.toString());
    return [];
  });
};

/**
 * Takes an array of table names and returns a promise for an object that has table names as keys and an alphabetically
 * sorted array of column names as their values.
 * @param {Array} tableNames Array of table names
 * @return {Promise} an object that has table names as keys and an alphabetically
 * @private
 */
TableManager.prototype._columnsForTables = function(tableNames) {
  // pg and/or Redshift doesn't seem to handle arrays as parameters for IN queries, so we have to expand the table names
  // to $1, $2, $3, [...]
  const dollars = _.times(tableNames.length, function (n) {
    return "$" + (n + 1);
  }).join(", ");

  return this._usingPg(this._logger, function (client) {
    return client.queryAsync(util.format("select tablename, \"column\" from pg_table_def where tablename in (%s) " +
                          "and schemaname = $%d", dollars, tableNames.length+1), tableNames.concat(this._defaultSchema))
      .then(function(res) {
        // res.rows is an array like [{tablename: "table1", column: "column1"},
        // {tablename: "table1", column: "column2}, ...]. Group it by tablename, then turn the resulting object into
        // an object like {table1: ["column1", "column2", ...], table2: ["column1", ...]}
        return _(res.rows).groupBy("tablename").transform(function(acc, v, k) {
          acc[k] = _.pluck(v, "column");
        }).value();
      })
  }.bind(this));
};

/**
 * Takes a table name, array of its columns and an array with the set of all column names for the view, returns a SELECT statement
 * that has columns in alphabetical order and missing columns as NULLs.
 * @param {String} tableName source table name
 * @param {Array} tableCols Array of columns the table has
 * @param {Array} allCols Union of column names for all time series tables in the view
 * @returns {String} select statement for table with columns in alphabetical order and missing columns as NULLs
 * @private
 */
TableManager.prototype._selectFor = function _selectFor(tableName, tableCols, allCols) {

  // which columns is tableName missing
  const missing = _.difference(allCols, tableCols)
    , colList = _(missing)
      // turn missing and present columns into helpful objects
      .map(function (col) {
        return {col: col, missing: true}
      })
      .concat(_.map(tableCols, function (col) {
        return {col: col, missing: false}
      }))
      // sort 'em by the column name
      .sort(function compare(a, b) {
        if (a.col < b.col) return -1;
        if (a.col > b.col) return 1;
        return 0;
      })
      // turn 'em into strings
      .map(function (obj) {
        if (obj.missing) {
          return "NULL as " + obj.col
        }
        return obj.col;
      })
      .join(", ")
    ;

  return util.format("select %s from %s", colList, tableName);
};

TableManager.prototype._dropTables = function(tables) {
  this._logger.info("Dropping tables");
  const query = "drop table if exists " + tables.join(",") + " cascade";
  return this._usingPg(this._logger, function(client) {
    return client.queryAsync(query).return(tables);
  });
};

/**
 * Prunes time series tables if necessary. Will return the names of time series tables that were spared
 * @param {Array} tables Array of table names
 * @param {String} name Base table name
 * @return {Promise} promise of array of spared time series tables
 * @private
 */
TableManager.prototype._pruneTsTables = Promise.method(function(name, tables) {
  this._logger.debug("_pruneTsTables for " + tables.length + " tables");

  if (!tables || !tables.length) {
    return [];
  }

  const tsOpts = this._options[name];
  if(tables.length > tsOpts.maxTables) {
    const diff = tables.length - tsOpts.maxTables
      , toDrop = _.take(tables, diff)
      , left = _.takeRight(tables, tsOpts.maxTables)
      ;

    this._logger.debug("Dropping " + diff + "/" + tables.length + " time series table(s) for "+ name + ". New amount " + left.length);
    return this._dropTables(toDrop).catch((err) => {
      this._logger.error("Error dropping " + toDrop.join(", ") + ": " + err);
    }).return(left);
  }
  return tables;
});

/**
 * @param {String} viewName name of view to create
 * @param {Array} viewTables array of tables to include in the view
 * @param {Object} columnMap a map of table name -> array of column names
 * @returns {Promise} A Promise that is fulfilled with the names of the tables in the view
 * @private
 */
TableManager.prototype._createView = Promise.method(function _createView(viewName, viewTables, columnMap) {

  if (!viewTables.length) {
    return [];
  }

  this._logger.debug("Creating view " + viewName + " with " + viewTables.length + " tables");
  // the set of all columns for all the tables in the view
  const allCols = _.union.apply(null, _.values(columnMap))
    , selects = _.map(viewTables, (table) => {
      return this._selectFor(table, columnMap[table], allCols);
    }).join(" union all ");

  // https://stackoverflow.com/questions/20692518/how-can-i-ensure-synchronous-ddl-operations-on-a-table-that-is-being-replaced
  return this._usingPg(this._logger, (client) => {
    this._logger.debug(util.format("_createView for %s starting new transaction for client %s", viewName, client.processID));
    return ut.inTransaction(client, "renaming " + viewName, (client) => {
      return client.queryAsync("drop view if exists old_" + viewName)
        .then(() => {
          return client.queryAsync(util.format("create view temp_%s as %s", viewName, selects))
        })
        .then(() => {
          return client.queryAsync(util.format("alter table %s rename to old_%s", viewName, viewName));
        })
        .then(() => {
          this._logger.debug("Renamed " + viewName);
          return client.queryAsync(util.format("alter table temp_%s rename to %s", viewName, viewName))
        })
        ;
    })
      .then(() => {
        this._logger.debug("Dropping old_" + viewName);
        return client.queryAsync("drop view if exists old_" + viewName)
      })
      .return(viewTables);
  });
});

/**
 * Creates rolling views that include at most the configured amount of tables for a given base name. The number
 * of view tables to include is gotten from the tablesInView (in the table config.)
 * @param {String} name base table name
 * @param {Array} tables array of all time series table names
 * @returns {Promise} promise of table names (equal to tables parameter)
 * @private
 */
TableManager.prototype._updateViews = Promise.method(function(name, tables) {
  const opts = this._options[name];

  if(!opts) {
    throw new Error(`No options for table ${name}`);
  }

  if(!tables.length) {
    return [];
  }

  let viewLengths = opts.tablesInView || [opts.maxTables];

  if (!_.isArray(viewLengths)) {
    if (_.isNumber(viewLengths)) {
      viewLengths = [viewLengths];
    } else {
      throw new Error(`tablesInView not an array or Number? ${inspect(viewLengths)}`);
    }
  }

  // get columns for all the tables that are used in views
  const columnMapP = this._columnsForTables(_.takeRight(tables, _.max(viewLengths)));

  return columnMapP.bind(this).then(function (columnMap) {
    return Promise.map(viewLengths, function (n) {
      const viewName = name + "_view_" + n + this._postfix;
      return this._createView(viewName, _.takeRight(tables, n), columnMap);
    }.bind(this));
  }).return(tables);
});

/**
 * Creates (if necessary) a time series table for a base name, and returns a promise of a table name.
 * @param {String} name base name for table. The "static" part of the table name, before any postfixes
 * @return {Promise} promise of table name
 */
TableManager.prototype.tableFor = Promise.method(function (name) {

  const tblConf = this._options[name];

  if(!tblConf) {
    throw new Error("Couldn't find table configuration for base " + name);
  }

  const nameWithPostf = this._defaultSchema + "." + name + this._postfix;

  // no time series table configuration, we're using a monolithic table
  if(!(tblConf.period || tblConf.maxTables || tblConf.tablesInView)) {
    return nameWithPostf;
  }

  const now = Date.now()
    , isoStr = new Date(((now - now % (tblConf.period * 1000)))).toISOString().split("T")
    ;

  let stampStr;
  if (tblConf.period < 86400) { // period is less than 24h
    stampStr = isoStr[0].split("-").join("") + "_" + isoStr[1].split(".")[0].split(":").slice(0, 2).join("");
  } else {
    stampStr = isoStr[0].split("-").join("_");
  }

  const tsTableName = nameWithPostf + "_ts_" + stampStr
      // not using IF NOT EXISTS since we need to know whether a new table was created or not without
      // having to do extra queries
      , ddlStr = util.format("CREATE TABLE %s (%s) %s ", tsTableName, tblConf.columns.join(',')
        , tblConf.tableAttrs.join(' '));

  return this._usingPg(this._logger, (client) => {
    return client.queryAsync(ddlStr)
  }).bind(this)
    .cancellable()
    .tap(() => {
      this._logger.info({tsTableName: tsTableName}, "Created new time series table");

      return this._listTsTables(name)
        .then(_.partial(this._updateViews.bind(this), name)).catch((err) => {
          this._logger.error({err: err, baseName: name}, "Error creating view");
          return [];
        })
        .then(_.partial(this._pruneTsTables.bind(this), name)).catch((err) => {
          this._logger.error({err: err, baseName: name}, "Error pruning time series tables");
        });
    })
    .catch(Promise.CancellationError, (e) =>{
      this._logger.error("tableFor cancelled");
      throw e;
    })
    .catch(isDuplicate, () => {
      this._logger.info({tsTableName: tsTableName}, "Time series table already exists");

      if(this._options._skipViewCreate) {
        return null;
      }

      return this._listTsTables(name)
        .then(_.partial(this._updateViews.bind(this), name)).catch((err) => {
          this._logger.error({err: err, baseName: name}, "Error creating view");
          return [];
        });
    })
    .return(tsTableName)
    .timeout(this._dbTimeout, "tableFor " + name)
    ;
});
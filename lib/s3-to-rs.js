/**
 * Created by teklof on 30.1.15.
 */

var $ = require('highland');
var _ = require('lodash');
var util = require('util');
var ut = require('./utils');
var Promise = require('bluebird');
var pause = require('promise-pauser');
var dbg = require('debug');
var _enabledOrig = dbg.enabled; // NOTE: temporarily force debug logging on
dbg.enabled = function(ns) {
  if(/s4qs/.test(ns)) return true; else return _enabledOrig(ns);
};
var debug = dbg('s4qs-rs:s3-to-rs');
var error = dbg('s4qs-rs:s3-to-rs:error');
error.log = console.error;
var LRU = require('lru-cache');

var mup = require('./manifest-uploader');
var Statsd = require('statsd-client');
var instr = require('./instrumentation');
var Uploader = mup.Uploader;
var inspect = _.partialRight(util.inspect, {depth: 2});

/**
 * S3Copier polls SQS for S3 object creation events and uses COPY to copy the S3 object data to Redshift.
 *
 * Don't forget to provide credentials in copyParams.
 *
 * @param {Object} s3 Initialized AWS S3 service object
 * @param {Object} pg Postgres module
 * @param {Object} rs Initialized AWS Redshift service object
 * @param {Object} poller SQS poller
 * @param {Number} options.pollIntervalSeconds Interval between SQS polls
 * @param {String} options.connStr Connection string for Redshift
 * @param {String} options.tablePostfix Added to the end of the table name generated by
 * copyParams.table. Handy for having different tables based on NODE_ENV
 * @param {Number} options.clusterAvailCheckInterval Check cluster status at clusterAvailCheckInterval ms intervals.
 * If the cluster is not available, SQS polling and manifest uploading will be stopped.
 * @param {Object} options.manifestUploader Manifest uploader options. See ./manifest-uploader.js for details.
 *
 * @param {Object} options.tableConfig Time series table configuration
 * @param {Number} options.tableConfig.period Time series table period in seconds. I.e. a value of 3600 would mean
 * a new time series table is created per every hour
 * @param {Number} options.tableConfig.maxTables Keep a maximum of maxTables time series tables. Oldest tables will be
 * deleted
 * @param {Object} options.tableConfig.tableDef Time series table DDL etc.
 * @param {Array} options.tableConfig.tableDef.columns Array of column definitions like ["source int not null encode bytedict",
 * "campaign char(24) encode bytedict", "..."]
 * @param {Array} options.tableConfig.tableDef.tableAttrs Array of table attributes like ["DISTKEY(ID)", "SORTKEY(TS)"]
 * @constructor
 * @type {S3Copier}
 */
var S3Copier = exports.S3Copier = function S3Copier(poller, pg, s3, rs, options) {
  if(!options) {
    throw new Error("Missing options");
  }

  if(!pg) {
    throw new Error("Missing PostgreSQL module");
  }

  if(!poller) {
    throw new Error("Missing poller");
  }

  if(!options.Redshift.connStr) {
    throw new Error("missing connection string");
  }

  if(!options.Redshift.params.ClusterIdentifier) {
    throw new Error("missing options.Redshift.params.ClusterIdentifier");
  }

  if(!options.manifestUploader) {
    throw new Error("missing manifest uploader options");
  }

  if(!rs) {
    throw new Error("Missing Redshift service");
  }

  if(!s3) {
    throw new Error("Missing S3 service")
  }

  if(!options.tableConfig) {
    throw new Error("Missing table configuration");
  }

  var copyParams = options.copyParams;

  var table = copyParams.table;
  if(!_.isString(table) && !_.isFunction(table)) {
    throw new Error("copyParams.table must be a string or function");
  }

  var _tableNamer;

  if(_.isString(table)) {
    _tableNamer = ut.tableStrToNamer(table);
  } else {
    _tableNamer = table;
  }

  this._options = options;

  this._pollIntervalS = options.pollIntervalSeconds || 60;

  this._availCheckInterval = options.Redshift.clusterAvailCheckInterval || -1;

  if(options.tablePostfix) {
    debug("Table postfix " + options.tablePostfix);
  }

  this._tablePostfix = options.tablePostfix || "";

  if(!copyParams.args) {
    copyParams.args = [];
  }

  copyParams.args.push("MANIFEST");

  this._commonCopyParams = copyParams;

  this._pauser = pause.pauser();
  this._pauser.pause();

  this._pg = pg;
  this._connStr = options.Redshift.connStr;
  this._baseToTempl = {};
  this._poller = poller;

  this._poller.messageStream.pause();

  this._rs = Promise.promisifyAll(rs);
  this.started = false;
  this._availChk = null; // used for cluster availability check interval

  // if an _onManifest call is running, this'll contain an object with properties that'll be fulfilled when the messages
  // have been handled. Keys will be table names
  this._manifestsPending = {};
  var lruOpts = options.LRU || {max: 1000};
  this._seenMsgs = LRU(lruOpts);

  this._dbTimeout = options.Redshift.timeout || (1000 * 60 * 15);
  //this._dbTimeout = 1000 * 30;

  var topStatsd = new Statsd(options.statsd);

  var baseTables = _.filter(_.keys(options.tableConfig), (key) => !!options.tableConfig[key].columns);

  // since _tableNamer takes an S3 URI and turns it into a table name, we can use it as the manifest uploader's grouper
  this._uploader = new Uploader(s3, _.merge(options.manifestUploader, {grouper: _tableNamer, statsd: options.statsd, baseTables: baseTables}));

  this._instrumenter = new instr.Instrumenter(topStatsd.getChildClient("s3-to-rs"));

  var duplicateIncr = this._instrumenter.instrumenterFn("increment", "duplicatemsgs");

  this._errInstr = new instr.Instrumenter(this._instrumenter, "errors");
  this._errIncr = this._errInstr.instrumenterFn("increment", "top");

  this._usingPg = (fn) => Promise.using(ut.getPgClient(this._pg, this._connStr, 50), fn);

  this._tblMgr = new TableManager(this._usingPg, copyParams.schema, this._tablePostfix, options.tableConfig);


  // handle duplicate messages
  this._seenStream = this._poller.messageStream.observe()
    .flatMap($.compose($, pause.waitFor(this._pauser)))
    .filter(this._isSeen.bind(this))
    .doto(duplicateIncr)
    .batchWithTimeOrCount(30000, 10).map((msgs) => {
      debug(`Deleting ${msgs.length} duplicates for URIs ${ut.messagesToURIs(msgs).join(",")}`);
      return $(this._poller.deleteMsgs(msgs));
    })
    .sequence()
    .errors((err) => {
      this._errIncr();
      error(`Error handling duplicate messages: ${err}`);
    })
  ;

  // i.e. at most as many parallel copies as the options say, default to number of base tables
  var maxParallel = options.copyParams.maxParallel || baseTables.length;

  debug(`Maximum number of parallel copies: ${maxParallel}`);

  // not duplicate
  this._poller.messageStream.fork()
/*    .doto(() => {
      debug("_poller.messageStream");
    })*/
    .flatMap($.compose($, pause.waitFor(this._pauser)))
    .filter(_.negate(this._isSeen.bind(this)))
/*    .doto(() => {
      debug("_poller.messageStream filter");
    })*/
    .pipe(this._uploader.inputStream);

  this._unseenStream = $(_.map(this._uploader.outputStreams, (stream, base) => {
    return stream.sequence()
      .filter(manif => manif.table == base)
      .map(this._onManifest.bind(this))
  })).merge()
    .mergeWithLimit(maxParallel)
    .errors((err) => {
    this._errIncr();
    error(`_unseenStream got an error: ${err}`);
    this.errorStream.write(err); // yeah yeah, ugly hack
  });

/*
  this._unseenStream = $(this._uploader.outputStreams)
    .merge()
    .sequence()
    .map(this._onManifest.bind(this))
    .parallel(maxParallel)
    .errors((err) => {
      this._errIncr();
      error(`_unseenStream got an error: ${err}`);
      this.errorStream.write(err); // yeah yeah, ugly hack
    })
/!*    .through(this._uploader.msgsToManifests.bind(this._uploader))
    .sequence()
    .map(this._onManifest.bind(this))
    .parallel(maxParallel)
    .errors((err) => {
      this._errIncr();
      error(`_unseenStream got an error: ${err}`);
      this.errorStream.write(err); // yeah yeah, ugly hack
    })*!/
  ;
*/

  this.errorStream = $();

};

S3Copier.prototype._shouldDedup = function _shouldDedup(base) {
  return !!this._dedupConfsFor(base);
};

S3Copier.prototype._dedupConfsFor = function _dedupConfsFor(base) {
  return this._options.tableConfig[base].deduplication;
};

S3Copier.prototype._cleanTable = function _cleanTable(client, table, base, latest) {
  var tableConfs = this._options.tableConfig[base];

  if (!tableConfs.deleteOlderThan) {
    return Promise.resolve(0);
  }

  var tsc = tableConfs.timestampCol
    , delOlder = tableConfs.deleteOlderThan
    ;

  debug(`Cleaning table ${table} for ${base}`);

  return client.queryAsync(`DELETE FROM ${table} WHERE ${tsc} < '${latest}'::timestamp - interval '${delOlder}'`)
    .get("rowCount")
    .tap((res) => debug(`Deleted ${res} rows from table ${table} (base ${base}), using latest date ${latest} - '${delOlder}'`))
    .timeout(this._dbTimeout, `_cleanTable table ${table} base ${base}`)
    ;
};

S3Copier.prototype._cleanDedup = function _cleanDedup(client, table, base) {
  var dedupConf = this._dedupConfsFor(base)
    , setAge = dedupConf.dedupSetAge || "1 day"
    ;

  debug(`Cleaning dedup table for ${base}`);

  return client.queryAsync(`DELETE FROM ${table}_dedup WHERE processing_time < sysdate - interval '${setAge}'`)
    .tap((res) => debug(`Deleted ${res.rowCount} rows from dedup table for ${base}`))
    .timeout(this._dbTimeout, `_cleanDedup table ${table} base ${base}`)
    ;
};

S3Copier.prototype._updateTracking = function _updateTracking(client, table, base, stagingTbl, latestFinalized) {
  var dedupConf = this._dedupConfsFor(base);
  debug(`Updating tracking table for ${base}`);
  return client.queryAsync(`INSERT INTO ${table}_tracking (processing_time, finalized_time) (
      SELECT sysdate, GREATEST(MAX(${this._options.tableConfig[base].timestampCol}) - interval '${dedupConf.dropOlderThan}', '${latestFinalized}'::timestamp) FROM ${stagingTbl})`);
};

S3Copier.prototype._updateDedupTbl = function _updateDedupTbl(client, table, base, stagingTbl, latestFinalized) {
  debug("Updating dedup table for " + base);
  var pk = this._tblMgr.pkFor(base).name
    , tsc = this._options.tableConfig[base].timestampCol
    , query = `INSERT INTO ${table}_dedup `
              + `(SELECT DISTINCT ${stagingTbl}.${pk}, sysdate as processing_time FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup USING (${pk}) `
              + `WHERE ${table}_dedup.${pk} IS NULL OR ${stagingTbl}.${tsc} < ('${latestFinalized}'::timestamp))`
    ;

  debug(query);
  return client.queryAsync(query).tap((res) => debug(`Inserted ${res.rowCount} rows into dedup table for ${base}`))
    .get("rowCount")
    .timeout(this._dbTimeout, `_updateDedupTbl table ${table} base ${base} staging ${stagingTbl}`)
    ;
};

S3Copier.prototype._badToErrTbl = function _badToStaging(client, table, base, stagingTbl, latestFinalized) {
  debug(`Inserting duplicates and old events into error table for ${base}`);
  var pk = this._tblMgr.pkFor(base).name
    , tsc = this._options.tableConfig[base].timestampCol
    , query = `INSERT INTO ${table}_errors `
              + `(SELECT ${table}_dedup.${pk}, sysdate as processing_time, ${stagingTbl}.* FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup USING (${pk}) `
              + `WHERE ${table}_dedup.${pk} IS NOT NULL OR ${stagingTbl}.${tsc} < ('${latestFinalized}'::timestamp))`
    ;

  return client.queryAsync(query).tap((res) => debug(`Found ${res.rowCount} duplicate/old rows for ${base}`))
    .get("rowCount")
    .timeout(this._dbTimeout, `_badToErrTbl table ${table} base ${base} staging ${stagingTbl}`)
    ;
};

function _quote(s) {
  return `"${s}"`
}

S3Copier.prototype._stagingToActual = function _stagingToActual(client, table, base, stagingTbl, latestFinalized) {
  debug(`Inserting from staging to actual table for ${base}`);

  var tableConfs = this._options.tableConfig[base]
    , columns = _(tableConfs.columns) // get column definitions
    // split each definition into words delimited by spaces, then get the first element of the resulting arrays
    .map(_.flow(_.partial(_.words, _, /\S+/g), _.property("0"), _quote)).valueOf()
    , pk = this._tblMgr.pkFor(base).name
    , query = `INSERT INTO ${table} SELECT ${columns.join(", ")} FROM `
              +
              `(SELECT ${stagingTbl}.*, ROW_NUMBER() OVER (PARTITION BY ${stagingTbl}.${pk}) AS id_rank, dedup.${pk} AS dedup_id `
              + `FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup AS dedup USING (${pk})) as staging `
              +
              `WHERE id_rank = 1 and dedup_id IS NULL AND staging.${tableConfs.timestampCol} >= ('${latestFinalized}'::timestamp)`
    ;
  debug(query);
  return client.queryAsync(query).tap((res) => debug(`Inserted ${res.rowCount} rows into actual table for ${base}`))
    .get("rowCount")
    .timeout(this._dbTimeout, `_stagingToActual table ${table} base ${base} staging ${stagingTbl}`);
};

S3Copier.prototype._copyTemplateFor = function(base) {
  if(!this._baseToTempl[base]) {
    // omit "table" key since overriding it would break ManifestUploader, which gets a namer function based on the table parameter
    // in its constructor, not per-manifest. TODO/FIXME: support per-table copyParams.table configurations in ManifestUploader
    var paramsForBase = _.omit(_.get(this._options.tableConfig[base], "copyParams") || {}, "table");
    var finalParams = _.defaultsDeep(_.cloneDeep(paramsForBase), _.cloneDeep(this._commonCopyParams));
    this._baseToTempl[base] = _copyParamsTempl(finalParams);
  }
  return this._baseToTempl[base];
};

S3Copier.prototype._connAndCopy = function _connAndCopy(s3URI, base, table) {

  var cancelCopy = null
    , txCtx = "COPY for " + s3URI
    ;

  var ins = new instr.Instrumenter(this._instrumenter, "connAndCopy." + base);


  function _copyTo(client, dest) {
    var query = util.format(this._copyTemplateFor(base), dest, s3URI);
    debug(`Starting COPY query ${query} for ${s3URI} to ${dest}`);
    return client.queryAsync(query).return(s3URI).timeout(this._dbTimeout, `COPY from ${s3URI} to ${dest}`)
      .tap(() => debug(`COPY for ${s3URI} to ${dest} done`));
  }

  var copyTo = ins.instrumentCalls("copyTo", _copyTo);

  function _dedupCopy(client) {
    debug(`Doing deduplication for base ${base}`);

    var dedupConf = this._dedupConfsFor(base);

    var okCounter = ins.usingInstrFn("counter", "okrows", _.identity)
      , badCounter = ins.usingInstrFn("counter", "badrows", _.identity)
      , oldDropCounter = ins.usingInstrFn("counter", "dropold", _.identity)

    // split table name (which includes the schema) by . and take last part since temporary tables have their own schema
    var stagingTbl = `${_.last(table.split("."))}_staging_${ut.randomString(5)}`;

    var stagingDdl = `CREATE TEMPORARY TABLE ${stagingTbl} (LIKE ${table})`;
    debug(`Creating staging table ${stagingTbl} LIKE ${table}`);

    var latestFinalizedP = client.queryAsync(
      (`SELECT COALESCE(MAX(finalized_time), sysdate - interval '${dedupConf.dropOlderThan}') as t FROM ${table}_tracking`))
      .then(_.flow(ut.get("rows"), ut.get(0), ut.get("t")))
      .timeout(this._dbTimeout, `Get last finalized timestamp for ${base}`);

    return latestFinalizedP.then(_.flow(ut.redshiftDateFix, ut.send("toISOString")))
      .tap((lf) => debug((`Latest finalization time for ${base} was ${lf}`)))
      .then((latestFinalized) => {
        return client.queryAsync(stagingDdl).timeout(this._dbTimeout, `Create staging table for ${base}`)
          .then(copyTo.bind(this, client, stagingTbl))
          .then(() => Promise.join(
            this._stagingToActual(client, table, base, stagingTbl, latestFinalized).tap(okCounter)
            , this._badToErrTbl(client, table, base, stagingTbl, latestFinalized).tap(badCounter)
          ))
          .then(() => Promise.join(
            this._updateDedupTbl(client, table, base, stagingTbl, latestFinalized)
            , this._updateTracking(client, table, base, stagingTbl, latestFinalized)
            , this._cleanDedup(client, table, base)
            , this._cleanTable(client, table, base, latestFinalized).tap(oldDropCounter)
          ))
      })
      .return(s3URI);
  }

  var dedupCopy = ins.instrumentCalls("dedupCopy", _dedupCopy);

  return this._usingPg(ins.instrumentCalls("total", (client) =>
      ut.inTransaction(client, txCtx, (txClient) => {
        var oldDropCounter = ins.usingInstrFn("counter", "dropold", _.identity);

        cancelCopy = () => ut.cancelClient(txClient);
        if (this._shouldDedup(base)) return dedupCopy.call(this, txClient);
        else return copyTo.call(this, txClient, table)
          .then(() => client.queryAsync(`SELECT MAX(${this._options.tableConfig[base].timestampCol}) AS t FROM ${table}`))
          .then(_.flow(ut.get("rows"), ut.get(0), ut.get("t"), ut.redshiftDateFix, ut.send("toISOString")))
          .then(_.partial(this._cleanTable.bind(this), client, table, base)).tap(oldDropCounter)
          .return(s3URI)
          ;
      })
    , {count: true}))
    .cancellable()
    .tap(() => {
      cancelCopy = null;
      debug("COPY to " + table + " done");
    })
    .catch(Promise.CancellationError, (e) => {
      if(cancelCopy) {
        cancelCopy();
      }

      error(`_connAndCopy cancelled for ${table} ${s3URI}`);
      throw e;
    });
};

S3Copier.prototype._markSeen = function(msgs) {
  _.each(ut.messagesToURIs(msgs), (uri) => this._seenMsgs.set(uri, true));
};

S3Copier.prototype._isSeen = function(msg) {
  // TODO(ORBAT): FIXME. This is pretty stupid, but as events _currently_ have only one URI/msg, I'm not overly worried
  return _.any(ut.messageToURIs(msg), (uri) => !!this._seenMsgs.get(uri));
};

S3Copier.prototype._dedup = function(msgs) {
  return new Promise((resolve) => {
    // bang bang to coerce undefined into a bool
    var grouped = _.groupBy(msgs, (msg) => !!this._seenMsgs.get(msg.MessageId));
    // -> {true: [duplicates], false: [nondups]}

    var dups = grouped.true || [];
    var nonDups = grouped.false || [];

    if (dups.length > 0) {
      error(`Found ${dups.length} duplicate messages. Deleting`);
      resolve(this._poller.deleteMsgs(dups)
        .catch((err) => error(`Error deleting duplicates: ${err}`))
        .tap(() => error("Duplicates deleted"))
        .return(nonDups))
      ;
    } else {
      resolve(nonDups);
    }

  });
};

S3Copier.prototype._delete = function(msgs) {
  debug(`Deleting ${msgs.length} messages`);
  return this._poller.deleteMsgs(msgs)
    .tap(() => debug("Messages deleted"));
};


S3Copier.prototype._onManifest = function _onManifest(manif) {
  debug(`Handling manifest ${manif.manifestURI} for base table ${manif.table}`);

  var handle = () => {
    var doCopy = _.partial(this._connAndCopy.bind(this), manif.manifestURI, manif.table);

    return this._tblMgr.tableFor(manif.table)
      .cancellable()
      .then(doCopy)
      .then((uri) => {
        debug(`Manifest ${uri} copied`);
        this._markSeen(manif.msgs);
        return this._delete(manif.msgs).catch((err) =>
          error(`Couldn't delete messages for manifest ${manif.manifestURI}: ${err}`));
      })
      .then(() =>
        manif.done(true).catch((err) =>
          error(`Couldn't move manifest ${manif.manifestURI}: ${err}`)))
      .catch((err) => {
        error(`Error handling manifest ${manif.manifestURI}: ${err}`);
        return manif.done(false).catch((err) => error(`Couldn't move manifest ${manif.manifestURI}: ${err}`)).tap(() => {
          if (err instanceof Promise.CancellationError || err instanceof Promise.TimeoutError) {
            throw err;
          }
        });
      })
      .return(manif)
    ;
  };

  var pendingPromise;

  if(this._manifestsPending[manif.table] && this._manifestsPending[manif.table].isPending()) {
    debug(`Already have a pending manifest upload for table ${manif.table}${this._tablePostfix}. ${manif.manifestURI} is waiting`);
    pendingPromise = this._manifestsPending[manif.table].then(handle);
  } else {
    pendingPromise = handle();
  }

  this._manifestsPending[manif.table] = pendingPromise;

  return $(pendingPromise.tap(() => debug(`Handled manifest ${manif.manifestURI} for base table ${manif.table}`)));

};

/**
 * Stop the S3Copier. Returns a promise that'll be resolved once all underway COPYs are done.
 * @param {Boolean} checkAvail If true, don't stop the cluster availability checker.
 * @return {*}
 */
S3Copier.prototype.stop = function(checkAvail) {
  if(this.started) {
    // set this so that if _availHandler is waiting for its promise it won't try to start S3Copier again when it sees
    // it isn't started.
    this._reallyStop = !checkAvail;
    this.started = false;

    debug("Stopping. checkAvail " + checkAvail);

    this._poller.stop();

    if(!this._pauser.paused) {
      this._pauser.pause();
    } else {
      error("Trying to stop when not actually running?! " + inspect(this._pauser));
    }

    if(!checkAvail) {
      clearInterval(this._availChk);
    }
  }

  if(this._numPending() > 0) {
    return Promise.settle(_.map(_.values(this._manifestsPending), (promise) => {
      if (promise.isPending()) {
        return promise.cancel();
      } // TODO: add timeout
      return null;
    }))
      .then((settled) => {
        var byFulfilled = _.groupBy(settled, ut.send("isFulfilled"));
        var nOk = _.isArray(byFulfilled.true) ? byFulfilled.true.length : 0;
        var nFail = _.isArray(byFulfilled.false) ? byFulfilled.false.length : 0;
        debug("Fulfilled after cancel: " + nOk);
        debug("Failed after cancel: " + nFail);
        if(nFail) {
          debug("Reasons: " + _.map(byFulfilled.false, ut.send("reason")));
        }
      });
  }

  return Promise.resolve({});
};

S3Copier.prototype._numPending = function _numPending() {
  return _.reduce(this._manifestsPending, (acc, val) => acc + (val.isPending() ? 1 : 0), 0);
};

S3Copier.prototype._availHandler = function _availHandler() {
  debug("Checking cluster availability");
  this._isClusterAvail()
    .then((avail) => {
      if (!avail) {
        if (this.started) {
          error("_availHandler: cluster went unavailable, bailing out");
          throw new Error("Cluster unavailable");
        } else {
          return Promise.resolve();
        }
      } else { // cluster available
        if (!this.started && !this._reallyStop) {
          error("_availHandler: cluster became available, starting");
          return this.start(true);
        } else {
          return Promise.resolve();
        }
      }
    });
};

S3Copier.prototype._isClusterAvail = function _isClusterAvail() {
  return this._rs.describeClustersAsync()
    .then((res) => {
      if (res.Clusters.length != 1) {
        throw new Error("DescribeClusters got " + res.Clusters.length + " results: did you forget to set " +
                        "Redshift.params.ClusterIdentifier?");
      }

      debug("ClusterStatus " + res.Clusters[0].ClusterStatus);
      return res.Clusters[0].ClusterStatus === "available";
    })
    .catch((err) => {
      error("_isClusterAvail got " + err + " when trying to check cluster availability");
      return false;
    });
};

/**
 * Starts the S3Copier
 * @param {Boolean} skipAvailCheck if skipAvailCheck is true, skip cluster availability check and just start.
 */
S3Copier.prototype.start = function start(skipAvailCheck) {
  if (!this.started) {
    return (skipAvailCheck ? Promise.resolve(true) : this._isClusterAvail())
      .then((avail) => {
        if (avail) {
          debug("Cluster available, ready to start");
          return Promise.join(this._tblMgr.createMetadataTbls(), this._tblMgr.createMonolithicTbls())
            .then(() => {
              if (!this._streamsInited) {
                // NOTE(ORBAT): hideous kluge to see explicitly thunking both streams helps with odd occasional freezing

                this._seenStream.each(function () {
                  debug("Duplicate deletion done");
                });

                this._unseenStream.each(function (mf) {
                  debug("Finished handling " + mf.manifestURI);
                });

                this._streamsInited = true;
              }
              this._pauser.unpause();
              // these resumes only need to happen once, but it won't hurt calling resume multiple times.
              // Once the streams get going, we can use _pauser to pause/unpause everything.
              this._seenStream.resume();
              this._unseenStream.resume();
              this._poller.start();
              this.started = true;

              if (this._availCheckInterval > 0 && !this._availChk) {
                this._availChk = setInterval(this._availHandler.bind(this), this._availCheckInterval * 1000);
              }

            })
            .tap(() => debug("Starting"));
        } else {
          error("Cluster unavailable, can't start yet");
          if (this._availCheckInterval < 0) {
            throw new Error("Cluster unavailable and no clusterAvailCheckInterval configured, bailing out");
          }

          // TODO: there's got to be a smarter way of doing this.
          // The interval can't be started before createMonolithicTbls and createMetadataTbls are finished, since
          // they might take longer than _availCheckInterval to complete, meaning start() would get called again
          // because we're not started yet
          if(!this._availChk) {
            this._availChk = setInterval(this._availHandler.bind(this), this._availCheckInterval * 1000);
          }


        }
      });
  }

  return Promise.resolve();
};

var _boolish = /^(true|on|false|off)$/i;
var _enc = /^encoding$/i;

var _copyParamsTempl = exports._copyParamsTempl = function _copyParamsTempl(copyParams) {

  copyParams.withParams = copyParams.withParams || {};
  copyParams.args = copyParams.args || [];

  var withp = _.reduce(_.pairs(copyParams.withParams), (acc, pair) => {
    var key = pair[0]
      , value = pair[1];
    if (!(_.isNumber(value) || value.toString().match(_boolish) || key.match(_enc))) {
      // we need quotes since value is not a number or boolean-ish, and the key isn't "encoding"
      value = "'" + value + "'";
    }
    acc.push(key + " " + value);

    return acc;

  }, []);

  return "COPY %s FROM '%s' " + copyParams.args.concat(withp).join(' ') + ";";
};

/**
 * Creates new time series tables if needed, creates views for the tables, and deletes old tables.
 * @param {Function} usingPgFn A function that takes another function which will be called with a promisified Postgres client
 * and returns a promise of a result
 * @param {String} postfix A table postfix to be added to the base name _before_ the time series postfix.
 * @constructor
 */
var TableManager = exports.TableManager = function TableManager(usingPgFn, schema, postfix, options) {

  if(!_.isString(postfix)) {
    throw new Error("postfix not a string");
  }

  if(!schema) {
    throw new Error("missing schema");
  }

  if(!_.isFunction(usingPgFn)) {
    throw new Error("Postgres client getter not a function");
  }

  if(!options) {
    throw new Error("no options");
  }

  this._options = options;

  this._baseNames = _.filter(_.keys(options), (key) => !!options[key].columns);

  this._defaultSchema = schema;

  this._postfix = postfix;

  this._usingPg = usingPgFn;

  this._dbTimeout = options.timeout || (10 * 60 * 1000);

};

function isDuplicate(error) {
  return error && error.code === "42P07";
}

var _pkRe = /\bprimary key\b/i;

/**
 * Returns the name and type of the primary key for a base table
 * @param {String} base base table name
 * @return {{name: String, type: String}}
 */
TableManager.prototype.pkFor = function(base) {
  var cols = this._options[base].columns;
  for(var i = 0; i < cols.length; ++i) {
    if(_pkRe.test(cols[i])) {
      var colParts = _.words(cols[i], /\S+/g);
      return {name: colParts[0], type: colParts[1]};
    }
  }

  throw new Error("Couldn't find a PRIMARY KEY column in tableConfig for " + base);
};

TableManager.prototype._createDedupTbl = function(client, base) {
  var pkInfo = this.pkFor(base)
    , baseSchema = _.get(this._options[base], "copyParams.schema")
    , schema = baseSchema ? baseSchema : this._defaultSchema
    , table = `${schema}.${base}${this._postfix}_dedup`;

  debug(`Creating dedup table ${table} with PK ${pkInfo.name} ${pkInfo.type}`);

  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (${pkInfo.name} ${pkInfo.type} DISTKEY, processing_time TIMESTAMP SORTKEY)`);
};

TableManager.prototype._createTrackingTbl = function(client, base) {
  var table = `${this._defaultSchema}.${base}${this._postfix}_tracking`;

  debug("Creating tracking table " + table);

  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (processing_time TIMESTAMP ENCODE DELTA, finalized_time TIMESTAMP SORTKEY)`);
};

TableManager.prototype._createErrorTbl = function(client, base) {
  var pkInfo = this.pkFor(base)
    , table = `${this._defaultSchema}.${base}${this._postfix}_errors`;

  debug(`Creating error table ${table} with PK ${pkInfo.name} ${pkInfo.type}`);
  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (dedup_${pkInfo.name} ${pkInfo.type}, processing_time TIMESTAMP, ${this._options[base].columns.join(", ")})`);

};

TableManager.prototype.createMonolithicTbls = Promise.method(function () {
  var monoTbls = _.filter(this._baseNames, (base) => {
    var opts = this._options[base];
    return !(opts.period && opts.maxTables && opts.tablesInView);
  });

  if(monoTbls.length === 0) {
    debug("No monolithic tables configured for any base table");
    return;
  }

  debug("Setting up monolithic tables for " + monoTbls.join(", "));

  return this._usingPg((client) =>
      ut.inTransaction(client, "createMonolithicTbls", (txClient) =>
        Promise.map(monoTbls, (base) => {
          var opts = this._options[base];
          var nameWithPostf = `${this._defaultSchema}.${base}${this._postfix}`;
          debug("Creating table " + nameWithPostf);
          return txClient.queryAsync(
            `CREATE TABLE IF NOT EXISTS ${nameWithPostf} (${opts.columns.join(',')}) ${opts.tableAttrs.join(' ')}`);
        })))
    .timeout(this._dbTimeout, "createMonolithicTbls");
});

TableManager.prototype.createMetadataTbls = Promise.method(function() {
  var tablesWithDedup = _.filter(this._baseNames, (base) => !!this._options[base].deduplication);

  if(tablesWithDedup.length == 0) {
    debug("No deduplication configured for any base table");
    return null;
  }

  debug("Setting up deduplication metadata tables for " + tablesWithDedup.join(", "));
  return this._usingPg((client) =>
    ut.inTransaction(client, "createMetadataTbls", (client) =>
      Promise.map(tablesWithDedup, (base) =>
        Promise.join(
          this._createTrackingTbl(client, base)
          , this._createErrorTbl(client, base)
          , this._createDedupTbl(client, base)
        ))))
    .timeout(this._dbTimeout, "createMetadataTbls");
});

/**
 * Finds time series tables for a base name and returns them in a promise of an array
 * @param name base name
 * @return {Promise} promise of array of table names, sorted oldest first
 * @private
 */
TableManager.prototype._listTsTables = function(name) {
  return this._usingPg(function (client) {
    var queryStr = "select tablename from pg_tables where schemaname = $1" +
                   " and tablename like $2 || '_ts_%' order by tablename asc"
      , params = [this._defaultSchema, name + this._postfix]
      ;

    return client.queryAsync(queryStr, params)
      .then(function (res) {
        return _.pluck(res.rows, "tablename");
      });
  }.bind(this)).catch(function(e) {
      error("_listTsTables error: " + e.toString());
      return [];
    });
};

/**
 * Takes an array of table names and returns a promise for an object that has table names as keys and an alphabetically
 * sorted array of column names as their values.
 * @param {Array} tableNames Array of table names
 * @return {Promise} an object that has table names as keys and an alphabetically
 * @private
 */
TableManager.prototype._columnsForTables = function(tableNames) {
  // pg and/or Redshift doesn't seem to handle arrays as parameters for IN queries, so we have to expand the table names
  // to $1, $2, $3, [...]
  var dollars = _.times(tableNames.length, function (n) {
    return "$" + (n + 1);
  }).join(", ");

  return this._usingPg(function (client) {
    return client.queryAsync(util.format("select tablename, \"column\" from pg_table_def where tablename in (%s) " +
                          "and schemaname = $%d", dollars, tableNames.length+1), tableNames.concat(this._defaultSchema))
      .then(function(res) {
        // res.rows is an array like [{tablename: "table1", column: "column1"},
        // {tablename: "table1", column: "column2}, ...]. Group it by tablename, then turn the resulting object into
        // an object like {table1: ["column1", "column2", ...], table2: ["column1", ...]}
        return _(res.rows).groupBy("tablename").transform(function(acc, v, k) {
          acc[k] = _.pluck(v, "column");
        }).value();
      })
  }.bind(this));
};

/**
 * Takes a table name, array of its columns and an array with the set of all column names for the view, returns a SELECT statement
 * that has columns in alphabetical order and missing columns as NULLs.
 * @param {String} tableName source table name
 * @param {Array} tableCols Array of columns the table has
 * @param {Array} allCols Union of column names for all time series tables in the view
 * @returns {String} select statement for table with columns in alphabetical order and missing columns as NULLs
 * @private
 */
TableManager.prototype._selectFor = function _selectFor(tableName, tableCols, allCols) {

  // which columns is tableName missing
  var missing = _.difference(allCols, tableCols)
    , colList = _(missing)
      // turn missing and present columns into helpful objects
      .map(function (col) {
        return {col: col, missing: true}
      })
      .concat(_.map(tableCols, function (col) {
        return {col: col, missing: false}
      }))
      // sort 'em by the column name
      .sort(function compare(a, b) {
        if (a.col < b.col) return -1;
        if (a.col > b.col) return 1;
        return 0;
      })
      // turn 'em into strings
      .map(function (obj) {
        if (obj.missing) {
          return "NULL as " + obj.col
        }
        return obj.col;
      })
      .join(", ")
    ;

  return util.format("select %s from %s", colList, tableName);
};

TableManager.prototype._dropTables = function(tables) {
  debug("Dropping tables " + tables);
  var query = "drop table if exists " + tables.join(",") + " cascade";
  return this._usingPg(function(client) {
    return client.queryAsync(query).return(tables);
  });
};

/**
 * Prunes time series tables if necessary. Will return the names of time series tables that were spared
 * @param {Array} tables Array of table names
 * @param {String} name Base table name
 * @return {Promise} promise of array of spared time series tables
 * @private
 */
TableManager.prototype._pruneTsTables = Promise.method(function(name, tables) {
  debug("_pruneTsTables for " + tables.length + " tables");

  if (!tables || !tables.length) {
    return [];
  }

  var tsOpts = this._options[name];
  if(tables.length > tsOpts.maxTables) {
    var diff = tables.length - tsOpts.maxTables
      , toDrop = _.take(tables, diff)
      , left = _.takeRight(tables, tsOpts.maxTables)
      ;

    debug("Dropping " + diff + "/" + tables.length + " time series table(s) for "+ name + ". New amount " + left.length);
    return this._dropTables(toDrop).catch(function(err) {
      error("Error dropping " + toDrop.join(", ") + ": " + err);
    }).return(left);
  }
  return tables;
});

/**
 * @param {String} viewName name of view to create
 * @param {Array} viewTables array of tables to include in the view
 * @param {Object} columnMap a map of table name -> array of column names
 * @returns {Promise} A Promise that is fulfilled with the names of the tables in the view
 * @private
 */
TableManager.prototype._createView = Promise.method(function _createView(viewName, viewTables, columnMap) {

  if (!viewTables.length) {
    return [];
  }

  debug("Creating view " + viewName + " with " + viewTables.length + " tables");
  // the set of all columns for all the tables in the view
  var allCols = _.union.apply(null, _.values(columnMap))
    , selects = _.map(viewTables, function (table) {
      return this._selectFor(table, columnMap[table], allCols);
    }.bind(this)).join(" union all ");

  // https://stackoverflow.com/questions/20692518/how-can-i-ensure-synchronous-ddl-operations-on-a-table-that-is-being-replaced
  return this._usingPg(function (client) {
    debug(util.format("_createView for %s starting new transaction for client %s", viewName, client.processID));
    return ut.inTransaction(client, "renaming " + viewName, function (client) {
      return client.queryAsync("drop view if exists old_" + viewName)
        .then(function() {
          return client.queryAsync(util.format("create view temp_%s as %s", viewName, selects))
        })
        .then(function() {
          return client.queryAsync(util.format("alter table %s rename to old_%s", viewName, viewName));
        })
        .then(function () {
          debug("Renamed " + viewName);
          return client.queryAsync(util.format("alter table temp_%s rename to %s", viewName, viewName))
        })
        ;
    })
      .then(function () {
        debug("Dropping old_" + viewName);
        return client.queryAsync("drop view if exists old_" + viewName)
      })
      .return(viewTables);
  });
});

/**
 * Creates rolling views that include at most the configured amount of tables for a given base name. The number
 * of view tables to include is gotten from the tablesInView (in the table config.)
 * @param {String} name base table name
 * @param {Array} tables array of all time series table names
 * @returns {Promise} promise of table names (equal to tables parameter)
 * @private
 */
TableManager.prototype._updateViews = Promise.method(function(name, tables) {
  var opts = this._options[name];

  if(!opts) {
    throw new Error(`No options for table ${name}`);
  }

  if(!tables.length) {
    return [];
  }

  var viewLengths = opts.tablesInView || [opts.maxTables];

  if (!_.isArray(viewLengths)) {
    if (_.isNumber(viewLengths)) {
      viewLengths = [viewLengths];
    } else {
      throw new Error(`tablesInView not an array or Number? ${inspect(viewLengths)}`);
    }
  }

  // get columns for all the tables that are used in views
  var columnMapP = this._columnsForTables(_.takeRight(tables, _.max(viewLengths)));

  return columnMapP.bind(this).then(function (columnMap) {
    return Promise.map(viewLengths, function (n) {
      var viewName = name + "_view_" + n + this._postfix;
      return this._createView(viewName, _.takeRight(tables, n), columnMap);
    }.bind(this));
  }).return(tables);
});

/**
 * Creates (if necessary) a time series table for a base name, and returns a promise of a table name.
 * @param {String} name base name for table. The "static" part of the table name, before any postfixes
 * @return {Promise} promise of table name
 */
TableManager.prototype.tableFor = Promise.method(function (name) {

  var tblConf = this._options[name];

  if(!tblConf) {
    throw new Error("Couldn't find table configuration for base " + name);
  }

  var nameWithPostf = this._defaultSchema + "." + name + this._postfix;

  // no time series table configuration, we're using a monolithic table
  if(!(tblConf.period || tblConf.maxTables || tblConf.tablesInView)) {
    return nameWithPostf;
  }

  var now = Date.now()
    , isoStr = new Date(((now - now % (tblConf.period * 1000)))).toISOString().split("T")
    ;

  var stampStr;
  if (tblConf.period < 86400) { // period is less than 24h
    stampStr = isoStr[0].split("-").join("") + "_" + isoStr[1].split(".")[0].split(":").slice(0, 2).join("");
  } else {
    stampStr = isoStr[0].split("-").join("_");
  }

  var tsTableName = nameWithPostf + "_ts_" + stampStr
      // not using IF NOT EXISTS since we need to know whether a new table was created or not without
      // having to do extra queries
      , ddlStr = util.format("CREATE TABLE %s (%s) %s ", tsTableName, tblConf.columns.join(',')
        , tblConf.tableAttrs.join(' '));

  return this._usingPg(function (client) {
    return client.queryAsync(ddlStr)
  }).bind(this)
    .cancellable()
    .tap(function () {
      debug("Created new time series table " + tsTableName);

      return this._listTsTables(name)
        .then(_.partial(this._updateViews.bind(this), name)).catch(function (err) {
          error("Error creating view for base name " + name + ": " + err.stack || err);
          return [];
        })
        .then(_.partial(this._pruneTsTables.bind(this), name)).catch(function (err) {
          error("Error pruning time series tables: " + err.stack || err);
        });
    })
    .catch(Promise.CancellationError, function(e) {
      error("tableFor cancelled");
      throw e;
    })
    .catch(isDuplicate, function () {
      debug("Time series table " + tsTableName + " already exists");

      if(this._options._skipViewCreate) {
        return null;
      }

      return this._listTsTables(name)
        .then(_.partial(this._updateViews.bind(this), name)).catch(function (err) {
          error("Error creating view for base name " + name + ": " + err.stack || err);
          return [];
        });
    })
    .return(tsTableName)
    .timeout(this._dbTimeout, "tableFor " + name)
    ;
});
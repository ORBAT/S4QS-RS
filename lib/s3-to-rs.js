/**
 * Created by teklof on 30.1.15.
 */
var _ = require('lodash');
var util = require('util');
var ut = require('./utils');
var Promise = require('bluebird');
var debug = require('debug')('s4qs-rs:s3-to-rs');
var error = require('debug')('s4qs-rs:s3-to-rs:error');
error.log = console.error;
var LRU = require('lru-cache');
var mup = require('./manifest-uploader');
var Uploader = mup.Uploader;
var inspect = _.partialRight(util.inspect, {depth: 10});

/**
 * S3Copier polls SQS for S3 object creation events and uses COPY to copy the S3 object data to Redshift.
 *
 * Don't forget to provide credentials in copyParams.
 *
 * @param {Object} s3 Initialized AWS S3 service object
 * @param {Object} pg Postgres module
 * @param {Object} rs Initialized AWS Redshift service object
 * @param {String|Function} copyParams.table String with name of table to COPY to, or a function that takes an
 * S3 URI and returns a table name
 * @param {Array} copyParams.args Array of strings to be given as parameterless arguments to COPY. For example
 * ['GZIP', 'TRUNCATECOLUMNS']
 * @param {Object} copyParams.withParams Object with parametrized COPY arguments. For example
 * {'TIMEFORMAT': 'auto', 'NULL': 'null'}
 * @param {Object} poller SQS poller
 * @param {Number} options.pollIntervalSeconds Interval between SQS polls
 * @param {String} options.connStr Connection string for Redshift
 * @param {String} options.tablePostfix Added to the end of the table name generated by
 * copyParams.table. Handy for having different tables based on NODE_ENV
 * @param {Number} options.clusterAvailCheckInterval Check cluster status at clusterAvailCheckInterval ms intervals.
 * If the cluster is not available,
 * @param {Object} options.manifestUploader Manifest uploader options. See ./manifest-uploader for details.
 * @constructor
 * @type {S3Copier}
 */
var S3Copier = exports.S3Copier = function S3Copier(poller, pg, s3, rs, copyParams, options) {
  if(!options) {
    throw new Error("Missing options");
  }

  if(!pg) {
    throw new Error("Missing PostgreSQL module");
  }

  if(!poller) {
    throw new Error("Missing poller");
  }

  if(!options.Redshift.connStr) {
    throw new Error("missing connection string");
  }

  if(!options.Redshift.params.ClusterIdentifier) {
    throw new Error("missing options.Redshift.params.ClusterIdentifier");
  }

  if(!options.manifestUploader) {
    throw new Error("missing manifest uploader options");
  }

  if(!rs) {
    throw new Error("Missing Redshift service");
  }

  if(!s3) {
    throw new Error("Missing S3 service")
  }

  var table = copyParams.table;
  if(!_.isString(table) && !_.isFunction(table)) {
    throw new Error("copyParams.table must be a string or function");
  }

  if(_.isString(table)) {
    this._tableNamer = _tableStrToNamer(table);
  } else {
    this._tableNamer = table;
  }

  this._pollIntervalS = options.pollIntervalSeconds || 60;

  this._availCheckInterval = options.clusterAvailCheckInterval || -1;

  if(options.tablePostfix) {
    debug("Table postfix " + options.tablePostfix);
    this._tablePostfix = options.tablePostfix; // will be added to the end of the table name
  }

  if(!copyParams.args) {
    copyParams.args = [];
  }

  copyParams.args.push("MANIFEST");

  this._pg = pg;
  this._connStr = options.Redshift.connStr;
  this._copyTemplate = _copyParamsTempl(copyParams);
  this._poller = poller;
  this._rs = Promise.promisifyAll(rs);
  this.started = false;
  this._timeout = null; // used for setTimeout obj
  this._availChk = null; // used for cluster availability check interval

  // this'll contain a promise that will be fulfilled when all pending manifests have been handled
//  this._manifestsPending = null;

  // if an _onMsg call is running, this'll contain an object with properties that'll be fulfilled when the messages
  // have been handled. Keys will be table names
  this._manifestsPending = {};
  var lruOpts = options.LRU || {max: 1000};
  this._seenMsgs = LRU(lruOpts);

  // since _tableNamer takes an S3 URI and turns it into a table name, we can use it as the manifest uploader's grouper
  this._uploader = new Uploader(s3, _.merge(options.manifestUploader, {grouper: this._tableNamer}));

};

S3Copier.prototype._connAndCopy = function _connAndCopy(s3URI, table) {
  var self = this;
  return new Promise(function (resolve, reject) {
    self._pg.connect(self._connStr, function (err, client, done) {
      if(err) {
        error("Error getting Redshift client: " + err);
        done();
        reject(err);
        return;
      }

      if(self._tablePostfix) {
        table += self._tablePostfix;
      }

      var query = util.format(self._copyTemplate, table, s3URI);

      client.query(query, function (err) {
        if(err) {
          done(client);
          reject(err);
          return;
        }
        done();
        resolve(s3URI);
      });
    });
  });
};

S3Copier.prototype._markSeen = function(msgs) {
  var self = this;
  _.each(msgs, function (msg) {
    self._seenMsgs.set(msg.MessageId, true);
  });
};

S3Copier.prototype._dedup = function(msgs) {
  var self = this;
  return new Promise(function (resolve) {
    var grouped = _.groupBy(msgs, function (msg) {
      return !!self._seenMsgs.get(msg.MessageId); // bang bang to coerce undefined into a bool
    }); // {true: [duplicates], false: [nondups]}

    var dups = grouped.true || [];
    var nonDups = grouped.false || [];

    if (dups.length > 0) {
      error("Found " + dups.length + " duplicate messages. Deleting");
      resolve(self._poller.deleteMsgs(dups)
        .catch(function (err) {
          error("Error deleting duplicates: " + err);
        })
        .tap(function() {
          debug("Duplicates deleted");
        })
        .return(nonDups))
      ;
    } else {
      resolve(nonDups);
    }

  });
};

S3Copier.prototype._delete = function(msgs) {
  debug("Deleting " + msgs.length + " messages");
  return this._poller.deleteMsgs(msgs)
    .catch(function (err) {
      error("Error deleting messages: " + err);
    })
    .tap(function () {
      debug("Messages deleted");
    });
};

S3Copier.prototype._onManifest = function _onManifest(manif) {
  debug("Handling manifest " + manif.manifestURI + " for table " + manif.table);
  var promise = this._connAndCopy(manif.manifestURI, manif.table).bind(this)
      .then(function (uri) {
        debug("Manifest " + uri + " copied");
        return this._delete(manif.msgs).catch(function() {
          error("Couldn't delete messages for manifest " + manif.manifestURI)
        });
      })
      .catch(function(err) {
        error("Error copying manifest " + manif.manifestURI + ": " + err);
      })
      .then(function() {
        return manif.delete().catch(function() {
          error("Couldn't delete manifest " + manif.manifestURI);
        });
      })
      .return(manif);

  if(this._manifestsPending[manif.table] && this._manifestsPending[manif.table].isPending()) {
    error("Already have a pending manifest upload for table " + manif.table + "? Joining it anyhow");
    promise = Promise.join(promise, this._manifestsPending[manif.table]);
  }
  this._manifestsPending[manif.table] = promise;

};

S3Copier.prototype._onMsgs = function _onMsgs(msgs) {
  debug("Got " + msgs.length + " messages");
  var nextPoll = Date.now() + this._pollIntervalS * 1000
    , self = this
    ;

  return new Promise(function (resolve) {
    if(msgs.length == 0) {
      debug("No messages, scheduling new poll.");
      self._schedulePoll(nextPoll);
      resolve();
      return;
    }

    resolve(self._dedup(msgs).then(function(nondups) {
      self._uploader.addMessages(nondups);
      self._schedulePoll(nextPoll);
    }));
  });
};

S3Copier.prototype._schedulePoll = function(nextPoll) {
  if(!this.started) {
    return; // bail out early
  }
  var timeLeft = nextPoll - Date.now();

  if(timeLeft < 0) {
    timeLeft = 0;
  }
  debug("Seconds until next poll: " + timeLeft / 1000);
  this._timeout = setTimeout(this._poller.poll.bind(this._poller) , timeLeft);
};

/**
 * Stop the S3Copier. Returns a promise that'll be resolved once all underway COPYs are done.
 * @param {Boolean} checkAvail If true, don't stop the cluster availability checker.
 * @return {*}
 */
S3Copier.prototype.stop = function(checkAvail) {
  if(this.started) {
    this.started = false;
    debug("Stopping");
    this._uploader.removeAllListeners("manifest");
    this._poller.removeAllListeners("messages");
    this._uploader.stop();
    clearTimeout(this._timeout);
    if(!checkAvail) {
      clearInterval(this._availChk);
    }
    if(this._manifestsPending) {
      debug(_.keys(this._manifestsPending).length + " pending manifest(s). Waiting for them to finish");
      return Promise.props(this._manifestsPending);
    }
  }
  return Promise.resolve({});
};

S3Copier.prototype._availHandler = function _availHandler() {
  this._isClusterAvail()
    .bind(this)
    .then(function (avail) {
      if (!avail) {
        if (this.running) {
          debug("_availHandler: cluster went unavailable, stopping");
          return this.stop(true);
        } else {
          return Promise.resolve();
        }
      } else { // cluster available
        if (!this.running) {
          debug("_availHandler: cluster became available, starting");
          return this.start(true);
        } else {
          return Promise.resolve();
        }
      }
    });
};

S3Copier.prototype._isClusterAvail = function _isClusterAvail() {
  return this._rs.describeClustersAsync().then(function (res) {
    if(res.Clusters.length != 1) {
      throw new Error("DescribeClusters got " + res.Clusters.length + " results: did you forget to set " +
                      "Redshift.params.ClusterIdentifier?");
    }

    debug("ClusterStatus " + res.Clusters[0].ClusterStatus);
    return res.Clusters[0].ClusterStatus === "available";
  });
};

/**
 * Starts the S3Copier
 * @param {Boolean} skipAvailCheck if skipAvailCheck is true, skip cluster availability check and just start.
 */
S3Copier.prototype.start = function start(skipAvailCheck) {
  if(!this.started) {
    (skipAvailCheck ? Promise.resolve() : this._isClusterAvail()).then(function (avail) {
      if(avail) {
        debug("Cluster available, starting");
        this._uploader.start();
        this._poller.on('messages', this._onMsgs.bind(this));
        this._uploader.on('manifest', this._onManifest.bind(this));
        this._poller.poll();
        this.started = true;
      } else {
        debug("Cluster unavailable. Waiting for it to become available before starting");
      }

    });
  }
};

var _boolish = /^(true|on|false|off)$/i;
var _enc = /^encoding$/i;

var _copyParamsTempl = exports._copyParamsTempl = function _copyParamsTempl(copyParams) {

  copyParams.withParams = copyParams.withParams || {};
  copyParams.args = copyParams.args || [];

  var withp = _.reduce(_.pairs(copyParams.withParams), function (acc, pair) {
    var key = pair[0]
      , value = pair[1];
    if (!(_.isNumber(value) || value.toString().match(_boolish) || key.match(_enc))) {
      // we need quotes since value is not a number or boolean-ish, and the key isn't "encoding"
      value = "'" + value + "'";
    }
    acc.push(key + " " + value);

    return acc;

  }, []);

  return "COPY %s FROM '%s' " + copyParams.args.concat(withp).join(' ') + ";";
};

/**
 * Takes a string and turns it into a function that returns a table name for an S3 URI. If the string starts with
 * a '/' it's assumed to be a regex, and the returned function will use that regex to build table names. If the
 * string doesn't start with a '/', the function will always return whatever 'table' contains.
 * @param {String} table string given in config
 * @return {Function} Function that takes an S3 URI and returns a table name
 * @private
 */
var _tableStrToNamer = exports._tableStrToNamer = function _tableStrToNamer(table) {

  if(_.first(table) === '/') { // happily assume it's a regex
    var lastSlash = _.lastIndexOf(table, '/')
      , pattern = _.initial(_.rest(table)).join('').substring(0,lastSlash - 1)
      , flags = table.substring(lastSlash + 1)
      , re = new RegExp(pattern, flags);

    debug("Using pattern " + pattern + " based on " + table);
    return _.partial(_URIToTbl, re);
  } else {
    return function() {
      return table;
    };
  }
};

var _URIToTbl = exports._URIToTbl = function(regex, uri) {
  var match = uri.match(regex);
  if(match && match[1]) {
    return match[1].split('.').join('_');
  } else {
    throw new Error("Can't turn '" + uri + "' into table name with regex " + regex);
  }
};
/**
 * Created by teklof on 30.1.15.
 */
const $ = require('highland');
const _ = require('lodash');
const util = require('util');
const config = require('config');
const ut = require('./utils');
const Promise = require('bluebird');
const aws = require('aws-sdk');
const pause = require('promise-pauser');
const ZabbixSender = require("zabbix-sender");
const LRU = require("lru-cache");
const co = require("co");

const p = require('./sqs-poller');
const mup = require('./manifest-uploader');
const Statsd = require('statsd-client');
const instr = require('./instrumentation');
const hc = require('./health-checker');
const Uploader = mup.Uploader;
const inspect = _.partialRight(util.inspect, {depth: 2});

/**
 * S3Copier polls SQS for S3 object creation events and uses COPY to copy the S3 object data to Redshift.
 *
 * Don't forget to provide credentials in copyParams.
 *
 * @param {Object} s3 Initialized AWS S3 service object
 * @param {Object} pg Postgres module
 * @param {Object} rs Initialized AWS Redshift service object
 * @param {Object} poller SQS poller
 * @param {Number} options.pollIntervalSeconds Interval between SQS polls
 * @param {Object} options.healthCheck Pipeline health check configuration
 * @param {String} options.connStr Connection string for Redshift
 * @param {String} options.tablePostfix Added to the end of the table name generated by
 * copyParams.table. Handy for having different tables based on NODE_ENV
 * @param {Number} options.clusterAvailCheckInterval Check cluster status at clusterAvailCheckInterval ms intervals.
 * If the cluster is not available, SQS polling and manifest uploading will be stopped.
 * @param {Object} options.manifestUploader Manifest uploader options. See ./manifest-uploader.js for details.
 *
 * @param {Object} options.tableConfig Time series table configuration
 * @param {Number} options.tableConfig.period Time series table period in seconds. I.e. a value of 3600 would mean
 * a new time series table is created per every hour
 * @param {Number} options.tableConfig.maxTables Keep a maximum of maxTables time series tables. Oldest tables will be
 * deleted
 * @param {Object} options.tableConfig.tableDef Time series table DDL etc.
 * @param {Array} options.tableConfig.tableDef.columns Array of column definitions like ["source int not null encode bytedict",
 * "campaign char(24) encode bytedict", "..."]
 * @param {Array} options.tableConfig.tableDef.tableAttrs Array of table attributes like ["DISTKEY(ID)", "SORTKEY(TS)"]
 * @constructor
 * @type {S3Copier}
 */
const S3Copier = exports.S3Copier = function S3Copier(pg, s3, rs, options) {
  if(!options) {
    throw new Error("Missing options");
  }

  if(!pg) {
    throw new Error("Missing PostgreSQL module");
  }

  if(!options.logger) {
    throw new Error("no logger");
  }

  if(!options.Redshift.connStr) {
    throw new Error("missing connection string");
  }

  if(!options.Redshift.params.ClusterIdentifier) {
    throw new Error("missing options.Redshift.params.ClusterIdentifier");
  }

  if(!options.manifestUploader) {
    throw new Error("missing manifest uploader options");
  }

  if(!rs) {
    throw new Error("Missing Redshift service");
  }

  if(!s3) {
    throw new Error("Missing S3 service")
  }

  if(!options.tableConfig) {
    throw new Error("Missing table configuration");
  }

  const copyParams = options.copyParams;

  const table = copyParams.table;
  if(!_.isString(table) && !_.isFunction(table)) {
    throw new Error("copyParams.table must be a string or function");
  }

  let _tableNamer;

  if(_.isString(table)) {
    _tableNamer = ut.tableStrToNamer(table);
  } else {
    _tableNamer = table;
  }

  this._tableNamer = _tableNamer;

  this._options = options;

  this._logger = options.logger.child({module: "S3Copier"});

  this._pollIntervalS = options.pollIntervalSeconds || 60;

  this._availCheckInterval = options.Redshift.clusterAvailCheckInterval || -1;

  if(options.tablePostfix) {
    this._logger.info({postfix: options.tablePostfix}, "Using a table postfix");
  }

  this._tablePostfix = options.tablePostfix || "";

  if(!copyParams.args) {
    copyParams.args = [];
  }

  copyParams.args.push("MANIFEST");

  this._commonCopyParams = copyParams;

  if(options.healthCheck) {
    this._logger.info({healthCheck: options.healthCheck}, "Pipeline health checking enabled");
    options.healthCheck.logger = this._logger;
    this._healthChecker = new hc.HealthChecker(options.healthCheck)
  }

  this._pauser = pause.pauser();
  this._pauser.pause();

  this._pg = pg;
  this._connStr = options.Redshift.connStr;
  this._baseToTempl = {};

  this._rs = Promise.promisifyAll(rs);
  this._started = false;
  this._availChk = null; // used for cluster availability check interval

  // if an _onManifest call is running, this'll contain an object with properties that'll be fulfilled when the messages
  // have been handled. Keys will be table names
  this._manifestsPending = {};

  this._dbTimeout = options.Redshift.timeout || (1000 * 60 * 15);

  const topStatsd = new Statsd(options.statsd);

  let zabbixSender;

  if(options.zabbix) {
    zabbixSender = new ZabbixSender(options.zabbix);
  }

  const baseTables = _.filter(_.keys(options.tableConfig), (key) => !!options.tableConfig[key].columns);

  this._URISeenCache = LRU(options.LRU || {max: 3000});

  this._uploader = new Uploader(s3, _.merge(options.manifestUploader, {logger: this._logger, statsd: options.statsd, baseTables: baseTables}));

  this._instrumenter = new instr.Instrumenter(topStatsd, "s3-to-rs", zabbixSender);

  this._errInstr = new instr.Instrumenter(this._instrumenter, "errors");
  this._errIncr = this._errInstr.instrumenterFn("increment", "top");

  this._usingPg = (logger, fn) => Promise.using(ut.getPgClient(this._pg, this._connStr, logger), fn);

  this._tblMgr = new TableManager(this._usingPg, copyParams.schema, this._tablePostfix, this._logger, options.tableConfig);

  this.errorStream = $();

  this._setupPollers(options);

};

S3Copier.prototype._setupPollers = function _setupPollers(options) {
  const errHandler = (err) => {
    // CancellationErrors need to be ignored so S3Copier.stop() can finish cleanup
    if (err instanceof Promise.CancellationError) {
      return;
    }

    this._errIncr();
    this._logger.error({err: err}, `_unseenStream encountered an unhandled error`);
    this.errorStream.write(err); // yeah yeah, ugly hack
  };

  const notSeen = msg => {
    const isSeen = _.any(msg.toURIs(), this._URISeenCache.get.bind(this._URISeenCache));
    return !isSeen;
  };

  const commonSqsConf = config.has('SQS') ? config.get('SQS') : {}
    , baseTables = _.filter(_.keys(options.tableConfig), (key) => !!options.tableConfig[key].columns);

  this._logger.info({baseTables: baseTables}, "Setting up SQS pollers");

  this._pollers = baseTables.map(baseName => {
    const baseSqsConf = options.tableConfig[baseName].SQS || {}
      , sqsConf = _.merge(baseSqsConf, commonSqsConf)
      , pollerOpts = sqsConf.poller || {};

    pollerOpts.filter = ut.nameFilterFnFor([baseName], this._tableNamer);
    pollerOpts.statsd = options.statsd;
    pollerOpts.logger = this._logger;

    this._logger.info({baseName: baseName}, "Creating SQS poller");

    const sqs = new aws.SQS({region: sqsConf.region, params: sqsConf.params})
      , poller = new p.Poller(sqs, pollerOpts);

    // stream messages from the poller to the manifest uploader's input stream for this base name

    poller.messageStream.fork()
      .flatMap($.compose($, pause.waitFor(this._pauser)))
      // only pass through messages that haven't been COPY'd already
      .filter(notSeen)
      // partially apply msgsToManifests with the baseName, resulting in a function that takes a stream, and then pass
      // that to through()
      .through(_.partial(this._uploader.msgsToManifests.bind(this._uploader), baseName))
      // msgsToManifests returns a Stream[Stream[Manifest]] (i.e. a stream of streams of manifests). "Unwrap" it
      // with sequence() so we end up with just a stream of Manifests
      .sequence()
      // feed the Manifests to _onManifest which returns a Promise of a Manifest. "Unwrap" that Promise with flatMap
      .flatMap(this._onManifest.bind(this))
      .errors(errHandler)
      .each((mf) => this._logger.info({baseName: baseName, manifestURI: mf.manifestURI}, "Finished handling manifest"))
    ;


    poller.messageStream.observe()
      // only pass through messages that _have_ been successfully COPY'd, i.e. duplicates
      .filter(msg => _.any(msg.toURIs(), this._URISeenCache.get.bind(this._URISeenCache)))
      .tap((msg) => this._logger.warn({uris: msg.toURIs()}, "Deleting message that was seen before"))
      .each(ut.send("deleteMsg"));

    return poller;
  });
};

S3Copier.prototype._shouldDedup = function _shouldDedup(base) {
  return !!this._dedupConfsFor(base);
};

S3Copier.prototype._dedupConfsFor = function _dedupConfsFor(base) {
  return this._options.tableConfig[base].deduplication;
};

S3Copier.prototype._cleanDedup = function _cleanDedup(client, table, base, latestFinalized) {
  const dedupConf = this._dedupConfsFor(base)
    , setAge = dedupConf.dedupSetAge || "1 day"
    ;

  client.txLogger.debug({baseName: base, dedupSetAge: setAge, latestFinalized: latestFinalized}, "Cleaning dedup table");

  const query = `DELETE FROM ${table}_dedup WHERE processing_time < '${latestFinalized}'::timestamp - interval '${setAge}'`;

  return client.queryAsync(query)
    .tap((res) => client.txLogger.debug({rowCount: res.rowCount, baseName: base}, `Dedup table cleaned`))
    .timeout(this._dbTimeout, `_cleanDedup table ${table} base ${base}`)
    ;
};

S3Copier.prototype._updateTracking = function _updateTracking(client, table, base, stagingTbl, latestFinalized) {
  if(this._healthChecker && !this._healthChecker.healthOk) {
    client.txLogger.warn({baseName: base}, `Pipeline health is bad, not updating finalization time`);
    return;
  }

  const dedupConf = this._dedupConfsFor(base);
  client.txLogger.debug({baseName: base, dropOlderThan: dedupConf.dropOlderThan, latestFinalized: latestFinalized}, "Updating tracking table");

  const query = `INSERT INTO ${table}_tracking (processing_time, finalized_time) (
      SELECT sysdate, GREATEST(MAX(${this._options.tableConfig[base].timestampCol}) - interval '${dedupConf.dropOlderThan}', '${latestFinalized}'::timestamp) FROM ${stagingTbl})`;

  return client.queryAsync(query);
};

S3Copier.prototype._updateDedupTbl = function _updateDedupTbl(client, table, base, stagingTbl) {
  const pk = this._tblMgr.pkFor(base).name
    , query = `INSERT INTO ${table}_dedup `
              + `(SELECT DISTINCT ${stagingTbl}.${pk}, sysdate as processing_time FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup USING (${pk}) `
              + `WHERE ${table}_dedup.${pk} IS NULL)`
    ;

  client.txLogger.debug({baseName: base, stagingTbl: stagingTbl}, "Updating dedup table");

  return client.queryAsync(query).tap((res) => client.txLogger.debug({rowCount: res.rowCount, baseName: base}, "Dedup table update done"))
    .get("rowCount")
    .timeout(this._dbTimeout, `_updateDedupTbl table ${table} base ${base} staging ${stagingTbl}`)
    ;
};

S3Copier.prototype._badToErrTbl = function _badToStaging(client, table, base, stagingTbl, latestFinalized) {
  client.txLogger.debug({baseName: base, stagingTbl: stagingTbl, latestFinalized: latestFinalized}, "Inserting duplicates and old events into error table");
  const pk = this._tblMgr.pkFor(base).name
    , tableConfs = this._options.tableConfig[base]
    , columns = columnNames(tableConfs.columns).map(_quote)
    , tsc = this._options.tableConfig[base].timestampCol
    , query = `INSERT INTO ${table}_errors (dedup_id, processing_time, ${columns.join(", ")}) `
              + `(SELECT ${table}_dedup.${pk} AS dedup_id, sysdate as processing_time, ${stagingTbl}.* FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup USING (${pk}) `
              + `WHERE ${table}_dedup.${pk} IS NOT NULL OR ${stagingTbl}.${tsc} < ('${latestFinalized}'::timestamp))`
    ;

  return client.queryAsync(query).tap((res) => client.txLogger.debug({rowCount: res.rowCount, baseName: base}, "Handled duplicate/old rows"))
    .get("rowCount")
    .timeout(this._dbTimeout, `_badToErrTbl table ${table} base ${base} staging ${stagingTbl}`)
    ;
};

/**
 * A function that takes a column definition (one row from tableConfig[base].columns) and returns the name of that column
 * in lower case.
 * Works by first splitting the definition into whitespace-separated words, taking the first element of that
 * array and then lowercasing it.
 * @param {String} Column definition like "ID CHAR(24) PRIMARY KEY ENCODE LZO"
 * @returns {String} Column name as string, like "id"
 */
const columnName = _.flow(_.partial(_.words, _, /\S+/g), _.property("0"), ut.send("toLowerCase"));

/**
 * Takes an array of column definitions (from tableConfig) and returns the names of the columns in lower case.
 * @param {Array} columnConfigs Array strings with column definitions like ["ID CHAR(24) PRIMARY KEY ENCODE LZO",
 * "SOURCE INT NOT NULL ENCODE BYTEDICT", ...]
 * @return {Array} Array of strings with column names like ["id", "source", ...]
 */
const columnNames = ut.splat(columnName);

function _quote(s) {
  return `"${s}"`
}

S3Copier.prototype._stagingToActual = function _stagingToActual(client, table, base, stagingTbl, latestFinalized) {
  client.txLogger.debug({baseName: base, stagingTbl: stagingTbl, latestFinalized: latestFinalized}, "Inserting from staging to actual table");

  const tableConfs = this._options.tableConfig[base]
    , columns = columnNames(tableConfs.columns).map(_quote)
    , pk = this._tblMgr.pkFor(base).name
    , query = `INSERT INTO ${table} SELECT ${columns.join(", ")} FROM `
              +
              `(SELECT ${stagingTbl}.*, ROW_NUMBER() OVER (PARTITION BY ${stagingTbl}.${pk}) AS id_rank, dedup.${pk} AS dedup_id `
              + `FROM ${stagingTbl} `
              + `LEFT JOIN ${table}_dedup AS dedup USING (${pk})) as staging `
              +
              `WHERE id_rank = 1 and dedup_id IS NULL AND staging.${tableConfs.timestampCol} >= ('${latestFinalized}'::timestamp)`
    ;

  return client.queryAsync(query).tap((res) => client.txLogger.debug({baseName: base, rowCount: res.rowCount}, "Inserted from staging to actual table"))
    .get("rowCount")
    .timeout(this._dbTimeout, `_stagingToActual table ${table} base ${base} staging ${stagingTbl}`);
};

S3Copier.prototype._copyTemplateFor = function(base) {
  if(!this._baseToTempl[base]) {
    const paramsForBase = _.get(this._options.tableConfig[base], "copyParams") || {};
    const finalParams = _.defaultsDeep(_.cloneDeep(paramsForBase), _.cloneDeep(this._commonCopyParams));
    this._baseToTempl[base] = _copyParamsTempl(finalParams);
  }
  return this._baseToTempl[base];
};

S3Copier.prototype._connAndCopy = function _connAndCopy(s3URI, base, table) {

  let cancelCopy
    , txCtx = "COPY for " + s3URI
    , logger = this._logger.child({baseName: base, table: table})
    , ins = new instr.Instrumenter(this._instrumenter, "connAndCopy." + base)
    ;

  logger.trace("Starting to copy data from S3");

  const _copyTo = (client, dest) => {
    const query = util.format(this._copyTemplateFor(base), dest, s3URI);
    client.txLogger.info({manifest: s3URI, dest: dest}, "Starting COPY query");
    return client.queryAsync(query).return(s3URI).timeout(this._dbTimeout, `COPY from ${s3URI} to ${dest}`)
      .tap(() => client.txLogger.info({manifest: s3URI}, "COPY done"));
  };

  const copyTo = ins.instrumentCalls("copyTo", _copyTo);

  const _dedupCopy = co.wrap(function*(client) {
    client.txLogger.info("Doing deduplication");

    const dedupConf = this._dedupConfsFor(base);

    const okCounter = ins.usingInstrFn("counter", "okrows", _.identity)
      , badCounter = ins.usingInstrFn("counter", "badrows", _.identity);

    // split table name (which includes the schema) by . and take last part since temporary tables have their own schema
    const stagingTbl = `${_.last(table.split("."))}_staging_${ut.randomString(5)}`;

    const stagingDdl = `CREATE TEMPORARY TABLE ${stagingTbl} (LIKE ${table})`;

    let finalizedResult = yield client.queryAsync(
      (`SELECT COALESCE(MAX(finalized_time), sysdate - interval '${dedupConf.dropOlderThan}') as t FROM ${table}_tracking`))
      .timeout(this._dbTimeout, `Get last finalized timestamp for ${base}`);


    const latestFinalized = ut.redshiftDateFix(_.get(finalizedResult, "rows[0].t")).toISOString();
    client.txLogger.trace({latestFinalized: latestFinalized}, "Found latest finalization time")

    client.txLogger.debug({stagingTbl: stagingTbl}, "Creating staging table");
    yield client.queryAsync(stagingDdl).timeout(this._dbTimeout, `Create staging table for ${base}`);
    yield copyTo(client, stagingTbl);

    // we can simultaneously move good rows from the staging table to the actual one, and bad rows to the errors table
    yield [this._stagingToActual(client, table, base, stagingTbl, latestFinalized).tap(okCounter)
      , this._badToErrTbl(client, table, base, stagingTbl, latestFinalized).tap(badCounter)];

    // these have to be done after the ones above since they alter the dedup table's contents, but they can still
    // run in parallel
    yield [this._updateDedupTbl(client, table, base, stagingTbl, latestFinalized)
      , this._updateTracking(client, table, base, stagingTbl, latestFinalized)
      , this._cleanDedup(client, table, base, latestFinalized)];

    client.txLogger.info("Deduplication done");

    return s3URI;
  }.bind(this));

  const dedupCopy = ins.instrumentCalls("dedupCopy", _dedupCopy);

  return this._usingPg(logger, ins.instrumentCalls("total", (client) =>
      ut.inTransaction(client, txCtx, (txClient) => {
        cancelCopy = () => ut.cancelClient(txClient);
        if (this._shouldDedup(base)) return dedupCopy(txClient);
        else return copyTo(txClient, table).return(s3URI);
      })
    , {count: true, zabbix: true}))
    .cancellable()
    .tap(() => {
      cancelCopy = null;
    })
    .catch(Promise.CancellationError, (e) => {
      if(cancelCopy) {
        logger.error({manifest: s3URI}, "Cancelling _connAndCopy");
        return cancelCopy().then(() => {throw e});
      }
    });
};

S3Copier.prototype._deleteMsgsFor = function(manif) {
  const msgs = manif.msgs;
  this._logger.debug({baseName: manif.baseName, nMsgs: msgs.length}, "Scheduling messages for deletion");
  if(this._logger.debug()) {
    const chunked = _.chunk(msgs, 20);
    _.each(chunked, chunk => this._logger.debug({uris: ut.messagesToURIs(chunk)}, "URIs scheduled for deletion"));
  }

  _.each(msgs, msg => msg.deleteMsg());
};

S3Copier.prototype._markAsSeen = function(manif) {
  _.each(manif.msgs, msg => _.each(msg.toURIs(), uri => this._URISeenCache.set(uri, true)));
};

S3Copier.prototype._onManifest = function _onManifest(manif) {
  const logger = this._logger.child({manifestURI: manif.manifestURI, baseName: manif.baseName});

  logger.info("Got new manifest");

  // replacing this with a co.wrap + generator function combo breaks cancelability, so this'll have to be as-is
  // pending a complete rewrite to a newer version of Bluebird with revamped cancellation
  const handle = () => {
    const tableName = this._tblMgr.tableFor(manif.baseName);

    return this._connAndCopy(manif.manifestURI, manif.baseName, tableName)
      .tap(() => this._markAsSeen(manif)) // mark all messages in a successfully copied manifest as seen
      .tap(() => {
        logger.info("Files in manifest copied to Redshift");
      })
      .then(() => manif.done(true).catch((err) =>
        logger.warn({err: err}, "Error moving manifest to success folder")))
      .then(() => {
        logger.debug("Deleting messages for OK manifest");
        this._deleteMsgsFor(manif);
      })
      .catch((err) => {
        logger.error({err: err}, "Error handling manifest");

        _.each(manif.msgs, m => m.stopVisibilityUpd());

        return manif.done(false).catch((err) => logger.warn({err: err}, "Couldn't move manifest to failure folder")).tap(() => {
          // if the error caught by the upper-level catch was a CancellationError or TimeoutError, cause this promise
          // to be rejected with it instead of ignoring it
          if (err instanceof Promise.CancellationError || err instanceof Promise.TimeoutError) {
            throw err;
          }
        });
      })
      .return(manif)
    ;
  };

  let pendingPromise;

  // NOTE: this shouldn't happen when using separate SQS queues
  if(this._manifestsPending[manif.baseName] && this._manifestsPending[manif.baseName].isPending()) {
    logger.fatal("Already have a pending manifest upload for this table? This shouldn't happen; bailing out");
    pendingPromise = this._manifestsPending[manif.baseName].then(handle);
  } else {
    pendingPromise = handle();
  }

  this._manifestsPending[manif.baseName] = pendingPromise;

  return $(pendingPromise);

};

/**
 * Stop the S3Copier. Returns a promise that'll be resolved once all underway COPYs are done.
 * @return {*}
 */
S3Copier.prototype.stop = function() {
  if(this._started) {
    // set this so that if _availHandler is waiting for its promise it won't try to start S3Copier again when it sees
    // it isn't started.
    this._stopping = true;
    this._started = false;

    this._logger.info("Stopping");

    _.each(this._pollers, p => p.stop());

    if(!this._pauser.paused) {
      this._pauser.pause();
    } else {
      this._logger.fatal({pauser: pauser}, "Trying to stop when not actually running?!");
    }

    clearInterval(this._availChk);
  }

  if(this._numPending() > 0) {
    return Promise.settle(_.values(this._manifestsPending).map((promise) => {
      if (promise.isPending()) {
        return promise.cancel();
      } // TODO: add timeout
      return null;
    }))
      .tap(() => {
        this._logger.info("Cancel done");
      });
  }

  return Promise.resolve({});
};

S3Copier.prototype._numPending = function _numPending() {
  return _.reduce(this._manifestsPending, (acc, val) => acc + (val.isPending() ? 1 : 0), 0);
};

S3Copier.prototype._availHandler = co.wrap(function* _availHandler() {
  this._logger.debug("Checking cluster availability");
  let avail = yield this._isClusterAvail();
  if (!avail) {
    if (this._started) {
      this._logger.fatal("_availHandler: cluster went unavailable, bailing out");
      throw new Error("Cluster unavailable");
    }
  } else { // cluster available
    if (!this._started && !this._stopping) {
      this._logger.info("_availHandler: cluster became available, starting");
      return this.start(true);
    }
  }
});

S3Copier.prototype._isClusterAvail = co.wrap(function* _isClusterAvail() {
  let describeResult;

  try {
    describeResult = yield this._rs.describeClustersAsync();
  } catch (err) {
    this._logger.error({err: err}, "Error checking cluster availability");
    return false;
  }

  if (describeResult.Clusters.length != 1) {
    throw new Error("DescribeClusters got " + describeResult.Clusters.length + " results: did you forget to set " +
                    "Redshift.params.ClusterIdentifier?");
  }

  this._logger.debug({ClusterStatus: describeResult.Clusters[0].ClusterStatus}, "Availability check done");

  return describeResult.Clusters[0].ClusterStatus === "available";
});

/**
 * Starts the S3Copier
 * @param {Boolean} skipAvailCheck if skipAvailCheck is true, skip cluster availability check and just start.
 */
S3Copier.prototype.start = co.wrap(function* start(skipAvailCheck) {
  if (this._started) {return Promise.resolve()}

  let avail = skipAvailCheck ? true : yield this._isClusterAvail();

  if (avail) {
    this._logger.info("Cluster available, ready to start");

    yield [this._tblMgr.createMetadataTbls(), this._tblMgr.createMonolithicTbls()];
    yield this._tblMgr.updateTableSchemas();

    this._pauser.unpause();
    _.each(this._pollers, p => p.start());
    this._started = true;

    if (this._availCheckInterval > 0 && !this._availChk) {
      this._availChk = setInterval(this._availHandler.bind(this), this._availCheckInterval * 1000);
    }

    if (this._healthChecker) {
      this._logger.info("Starting pipeline health checker");
      yield this._healthChecker.start();
    }

    this._logger.info("Started")

  } else {
    this._logger.error("Cluster unavailable, can't start yet");
    if (this._availCheckInterval < 0) {
      throw new Error("Cluster unavailable and no clusterAvailCheckInterval configured, bailing out");
    }

    // The interval can't be started before createMonolithicTbls and createMetadataTbls are finished, since
    // they might take longer than _availCheckInterval to complete, meaning start() would get called again
    // because we're not started yet
    if (!this._availChk) {
      this._availChk = setInterval(this._availHandler.bind(this), this._availCheckInterval * 1000);
    }
  }
});

const _boolish = /^(true|on|false|off)$/i;
const _enc = /^encoding$/i;

const _copyParamsTempl = exports._copyParamsTempl = function _copyParamsTempl(copyParams) {

  copyParams.withParams = copyParams.withParams || {};
  copyParams.args = copyParams.args || [];

  const withp = _.pairs(copyParams.withParams).reduce((acc, pair) => {
    const key = pair[0];
    let value = pair[1];
    if (!(_.isNumber(value) || value.toString().match(_boolish) || key.match(_enc))) {
      // we need quotes since value is not a number or boolean-ish, and the key isn't "encoding"
      value = "'" + value + "'";
    }
    acc.push(key + " " + value);

    return acc;

  }, []);

  return "COPY %s FROM '%s' " + copyParams.args.concat(withp).join(' ') + ";";
};

/**
 * Creates new time series tables if needed, creates views for the tables, and deletes old tables.
 * @param {Function} usingPgFn A function that takes another function which will be called with a promisified Postgres client
 * and returns a promise of a result
 * @param {String} postfix A table postfix to be added to the base name _before_ the time series postfix.
 * @constructor
 */
const TableManager = exports.TableManager = function TableManager(usingPgFn, schema, postfix, logger, options) {

  if(!_.isString(postfix)) {
    throw new Error("postfix not a string");
  }

  if(!schema) {
    throw new Error("missing schema");
  }

  if(!_.isFunction(usingPgFn)) {
    throw new Error("Postgres client getter not a function");
  }

  if(!logger) {
    throw new Error("No logger");
  }

  if(!options) {
    throw new Error("no options");
  }

  this._options = options;
  this._enableSchemaEvolution = !!options.enableSchemaEvolution;


  this._baseNames = _.filter(_.keys(options), (key) => !!options[key].columns);

  this._defaultSchema = schema;

  this._postfix = postfix;

  this._usingPg = usingPgFn;

  this._logger = logger.child({module: "TableManager"});

  this._dbTimeout = options.timeout || (10 * 60 * 1000);

  this._tablesWithDedup = _.filter(this._baseNames, (base) => !!this._options[base].deduplication);
  this._dedupEnabled = this._tablesWithDedup.reduce((acc, name) => {acc[name] = true; return acc}, {});
};

const _pkRe = /\bprimary key\b/i;

/**
 * Returns the name and type of the primary key for a base table
 * @param {String} base base table name
 * @return {{name: String, type: String}}
 */
TableManager.prototype.pkFor = function(base) {
  const cols = this._options[base].columns;
  for(let i = 0; i < cols.length; ++i) {
    if(_pkRe.test(cols[i])) {
      const colParts = _.words(cols[i], /\S+/g);
      return {name: colParts[0], type: colParts[1]};
    }
  }

  throw new Error("Couldn't find a PRIMARY KEY column in tableConfig for " + base);
};

TableManager.prototype._hasDedup = function(base) {
  return !!this._dedupEnabled[base];
};

TableManager.prototype._createDedupTbl = function(client, base) {
  const pkInfo = this.pkFor(base)
    , table = `${this.tableFor(base)}_dedup`;

  client.txLogger.info({dedupTable: table}, `Creating dedup table`);

  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (${pkInfo.name} ${pkInfo.type} DISTKEY, processing_time TIMESTAMP SORTKEY)`);
};

TableManager.prototype._createTrackingTbl = function(client, base) {
  const table = `${this.tableFor(base)}_tracking`;

  client.txLogger.info({trackingTable: table}, "Creating tracking table");

  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (processing_time TIMESTAMP ENCODE DELTA, finalized_time TIMESTAMP SORTKEY)`);
};

TableManager.prototype._createErrorTbl = function(client, base) {
  const pkInfo = this.pkFor(base)
    , table = `${this.tableFor(base)}_errors`;

  client.txLogger.info({errorTable: table}, `Creating error table`);
  return client.queryAsync(`CREATE TABLE IF NOT EXISTS ${table} (dedup_${pkInfo.name} ${pkInfo.type}, processing_time TIMESTAMP, handled_ts TIMESTAMP, ${this._options[base].columns.join(", ")})`);

};

TableManager.prototype.createMonolithicTbls = Promise.method(function () {
  const monoTbls = _.filter(this._baseNames, (base) => {
    const opts = this._options[base];
    return !(opts.period && opts.maxTables && opts.tablesInView);
  });

  if(monoTbls.length === 0) {
    this._logger.info("No monolithic tables configured for any base table");
    return;
  }

  this._logger.info({monoTbls: monoTbls}, "Creating monolithic tables if they don't exist");

  return this._usingPg(this._logger, (client) =>
      ut.inTransaction(client, "createMonolithicTbls", (client) =>
        Promise.map(monoTbls, (base) => {
          const opts = this._options[base];
          const nameWithPostf = `${this._defaultSchema}.${base}${this._postfix}`;
          client.txLogger.info({tableName: nameWithPostf}, "Creating table");
          return client.queryAsync(
            `CREATE TABLE IF NOT EXISTS ${nameWithPostf} (${opts.columns.join(',')}) ${opts.tableAttrs.join(' ')}`);
        })))
    .timeout(this._dbTimeout, "createMonolithicTbls");
});

TableManager.prototype.createMetadataTbls = Promise.method(function() {

  if(this._tablesWithDedup.length == 0) {
    this._logger.info("No deduplication configured for any base table");
    return null;
  }

  this._logger.info({tablesWithDedup: this._tablesWithDedup}, "Setting up deduplication metadata tables");
  return this._usingPg(this._logger, (client) =>
    ut.inTransaction(client, "createMetadataTbls", (client) =>
      Promise.map(this._tablesWithDedup, (base) =>
        Promise.join(
          this._createTrackingTbl(client, base)
          , this._createErrorTbl(client, base)
          , this._createDedupTbl(client, base)
        ))))
    .timeout(this._dbTimeout, "createMetadataTbls");
});

/**
 * Takes an array of base names and returns a promise for an object that has base names as keys and an array of lower case
 * column names as their values.
 * @param {Array} baseNames Array of base names
 * @return {Promise} an object that has base names as keys and arrays of column names as values:
 * {"my_table": ["id","ts",...], "other_table": ["column", "col2", ...], ...}
 * @private
 */
TableManager.prototype._columnsForTables = function(baseNames) {
  // add the postfix to base names
  const tableNames = baseNames.map(name => `${name}${this._postfix}`)
    // pg and/or Redshift doesn't seem to handle arrays as parameters for IN queries, so we have to expand the table names
    // to $1, $2, $3, [...]
    , dollars = _.times(tableNames.length, n => "$" + (n + 1)).join(", ")
    , postfRE = new RegExp(`${this._postfix}$`)
    ;

  return this._usingPg(this._logger, (client) => {
    return client.queryAsync(`select tablename, "column" from pg_table_def where tablename in (${dollars}) and schemaname = $${tableNames.length + 1}`, tableNames.concat(this._defaultSchema))
      .then(res => {
        // res.rows is an array like [{tablename: "table1", column: "column1"},
        // {tablename: "table1", column: "column2}, ...]. Group it by tablename, then turn the resulting object into
        // an object like {table1: ["column1", "column2", ...], table2: ["column1", ...]}
        return _(res.rows).groupBy("tablename").transform((acc, v, tableName) => {
          const columns = _.pluck(v, "column");
          // remove the postfix from the returned table name and lowercase column names
          acc[tableName.replace(postfRE, "")] = columns.map(ut.send("toLowerCase"))
        }).value();
      })
  });
};

TableManager.prototype._findColumnDef = function(baseName, colName) {
  const lowerCaseCol = colName.toLowerCase();
  return _.head(this._options[baseName].columns.filter(colDef => columnName(colDef).toLowerCase() == lowerCaseCol));
};

TableManager.prototype._genDropColumns = function(baseName, toDrop) {
  const monolTableName = this.tableFor(baseName);

  let dropSetup = toDrop.map(colName => [monolTableName, colName]);

  if(this._hasDedup(baseName)) {
    const errorsTableName = monolTableName + "_errors";
    dropSetup = dropSetup.concat(toDrop.map(colName => [errorsTableName, colName]));
  }

  return dropSetup.map(([tbl, col]) => `ALTER TABLE ${tbl} DROP COLUMN "${col}"`);
};

TableManager.prototype._genAddColumns = function(baseName, toAdd) {
  const monolTableName = this.tableFor(baseName);

  let dropSetup = toAdd.map(colName => [monolTableName, colName]);

  if(this._hasDedup(baseName)) {
    const errorsTableName = monolTableName + "_errors";
    dropSetup = dropSetup.concat(toAdd.map(colName => [errorsTableName, colName]));
  }

  return dropSetup.map(([tbl, col]) => `ALTER TABLE ${tbl} ADD COLUMN ${this._findColumnDef(baseName, col)}`)
};

/**
 * Generates ALTER statements for columns that were either removed from or added to the config. Returns an object
 * with base names as keys and arrays of ALTER statements as values.
 * @param {Object} allColsInDb. Object with base names as keys and lowercase column names (as string arrays) as values
 * @returns an object with base names as keys and arrays of ALTER statements as values: {"my_table":
 * ["ALTER TABLE my_table ADD COLUMN DERP INT", "ALTER TABLE my_table DROP COLUMN DOI", ...]}
 * @private
 */
TableManager.prototype._generateAlters = co.wrap(function* () {
  const allColsInDb = yield this._columnsForTables(this._baseNames);

  return _.transform(allColsInDb, (acc, dbCols, baseName) => {
    const columnConfs = this._options[baseName].columns
      , confNames = columnNames(columnConfs)
      , missingFromDb = _.difference(confNames, dbCols) // these need to be added to the db
      , removedFromConf = _.difference(dbCols, confNames) // these need to be removed from the db
      ;

    const drops = this._genDropColumns(baseName, removedFromConf)
      , adds = this._genAddColumns(baseName, missingFromDb)
      , alters = drops.concat(adds)
    ;

    if (alters.length) {
      this._logger.debug({baseName: baseName, nAdds: adds.length, nDrops: drops.length}, "Generated ALTER statements");
      acc[baseName] = alters;
    }
  });

});

TableManager.prototype._executeStatements = function(statements) {
  if(!this._enableSchemaEvolution) {
    this._logger.warn({statements: statements}, "Schema evolution is not enabled: nothing will actually be executed");
    return Promise.resolve();
  }

  return this._usingPg(this._logger, client =>
    ut.inTransaction(client, "updateTableSchemas", txClient =>
      ut.sequence(statements.map(stmt =>
        () => { // NOTE: we're returning a function here since ut.sequence works with Promise-returning functions
          txClient.logger.info({statement: stmt}, "Executing statement");
          return txClient.queryAsync(stmt).tap(() => txClient.logger.info({statement: stmt}, "Statement executed"));
        })
      ))
  );
};

/**
 * Checks the configs of all tables against the database and issues ALTERs as necessary.
 * @return {Promise} Promise that will be fulfilled with an array of altered base names when all ALTERs are done.
 */
TableManager.prototype.updateTableSchemas = co.wrap(function* () {
  const alters = yield this._generateAlters()
    , toBeAltered = Object.keys(alters)
  ;

  if(toBeAltered.length) {
    this._logger.info({toBeAltered: toBeAltered}, "Found tables that need ALTERing");
    try {
      // run all the statements in the alters object's values
      yield Promise.all(_.map(alters, (statements, baseName) => {
        this._logger.info({statements: statements, baseName: baseName}, "About to execute ALTERs");
        return this._executeStatements(statements);
      }));
    } catch (e) {
      this._logger.error({err: e}, "Error running ALTERs, bailing out");
      throw e;
    }
    this._logger.info({altered: toBeAltered}, "All ALTERs done");
    return toBeAltered;
  }
});
/**
 * Returns a fully qualified table name, i.e. including the schema and possible postfix
 * @param {String} name base name for table. The "static" part of the table name, before any postfixes
 * @return {String} fully qualified table name
 */
TableManager.prototype.tableFor = function (name) {
  return this._defaultSchema + "." + name + this._postfix;
};